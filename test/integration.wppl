/**
 * @fileoverview Some integration tests are defined in this file.
 * They're specified in an array below; each element is a complete
 * specification of a cognitive game. They are roughly ordered in
 * increasing order wrt to complexity.
 *
 * From the commandline, run
 $ webppl test/integration.wppl --require ../webppl-cognitive-agents
 */
let tests = function() {

  let testsArray = [
    /**
     * FIRST TEST
     * a trivial one-player game with a single action, the same at every
     * state and trivial rewards (only physical, unit at each state)
     */
    function() {
      let makeGame = function () {
        let actions = getConstantFn(["bottom"])

        let transitionFn = function (state, action) {
          return Delta({v: state + 1})
        }

        let API = function () {
          let getPreviousState = function (state) {
            return state - 1
          }
          let getLastAction = function (state) {
            assert(!isInitial(state));
            return 'bottom'
          }
          let endsRound = function(state, action) {
            return true
          }
          let isInitial = function (state) {
            return state == 0
          }
          let turn = getConstantFn(0)
          /** since there's only one player */
          let other = function (player) {
            return (player + 1) % 2
          }
          let stateToString = identity
          let API = {
            getPreviousState,
            getLastAction,
            endsRound,
            isInitial,
            turn,
            other,
            stateToString
          }
          return API
        }()

        let physicalRewardStructure = function () {
          let stateRewards = function (state) {
            return [[1]]
          }
          let actionRewards = function (state, action) {
            return [[0]]
          }
          return {
            actionRewards,
            stateRewards,
            quantity: 1
          }
        }()

        /** No mental rewards for simplicity */
        let mentalStateDynamics = function () {
          return {
            estimationHeuristicArr: [],
            mentalStateArr: [],
            mentalUtilities: [[]]
          }
        }()

        let initialState = 0

        let params = {
          beliefRepresentation: 'dirichlet',
          numberOfAgents: 1,
          numberOfRewards: {
            physical: 1,
            mental: 0
          }
        }

        let rewardUtilityFunctions = {
          physical: [identity],
          mental: []
        }

        return {
          actions,
          transitionFn,
          initialState,
          physicalRewardStructure,
          mentalStateDynamics,
          rewardUtilityFunctions,
          API,
          params
        }
      }

      /** Define a basic scenario */
      let scenarios = function() {
        let agent = {
          params: {
            goalCoeffs: [1],
            metaParams: {
              alpha: 100,
              discountFactor: 0.8,
              lookAhead: 2
            }
          },
          initialState: {
            belief: [null],
            mentalEstimations: [null],
            metaParamsEstimations: {
              alpha: [null],
              lookAhead: [null],
              discountFactor: [null]
            }
          }
        }

        let result = [
          {
            name: 'basic',
            agents: [ agent ],
            options: {
              horizon: 10,
              beliefRepresentation: 'dirichlet'
            }
          },
          {
            name: 'basic with discrete belief',
            agents: [ agent ],
            options: {
              horizon: 10,
              beliefRepresentation: 'discrete'
            }
          }
        ]

        return result
      }()

      let agents = ['bobby']

      let description = 'Trivial test with single state, single agent ' +
        'and single action, but repeated.'

      let validateOutput = function(trajectoriess) {
        info('validateOutput(): trajectoriess = ' + toString(trajectoriess))
        assertIsArray(trajectoriess, ARRAY_TYPE, 2,
          "FAIL: trajectoriess array should have length 2 (equal to " +
            "the number of scenarios) and have arrays as elements, but found: "
            + toString(trajectoriess))
        map(function(trajectories) {
          assertIsArray(trajectories, ARRAY_TYPE, 1,
              'FAIL: there should be a single trajectory for each ' +
              'scenario (as number of reps is set to 1), so trajectories ' +
              'array should have a single element, but found ' +
              toString(trajectories))
          const trajectory = trajectories[0]
          assertIsArray(trajectory, ARRAY_TYPE, 11,
            "FAIL: trajectory: " + toString(trajectories) + " not as expected")
          mapIndexed(function (i, elem) {
            assertEqual(elem, [i, 'bottom'])
          }, trajectory.slice(0, trajectory.length - 1))
        }, trajectoriess)
      }

      return {
        makeGame,
        scenarios,
        agents,
        description,
        validateOutput
      }
    }(),

    /**
     * SECOND TEST
     * a one-player game where at each state two actions are available:
     * (i) good and (ii) bad, with the former resulting in positive reward,
     * while the latter negative reward (physical only). Expect
     */
    function() {
      let makeGame = function () {
        let actions = function (state) { return ['bad', 'good'] }

        let transitionFn = function (state, action) {
          let nextState = [action].concat(state)
          return Delta({v: nextState})
        }

        let API = function () {
          let getPreviousState = function (state) {
            return state.slice(1)
          }
          let getLastAction = function (state) {
            assert(!isInitial(state));
            return state[0]
          }
          let endsRound = function (state, action) {
            return true
          }
          let isInitial = function (state) {
            return state.length == 0
          }
          let turn = getConstantFn(0)
          /** since there's only one player */
          let other = function (player) {
            return (player + 1) % 2
          }
          let stateToString = arrayToString
          let API = {
            getPreviousState,
            getLastAction,
            endsRound,
            isInitial,
            turn,
            other,
            stateToString
          }
          return API
        }()

        let physicalRewardStructure = function () {
          let stateRewards = function (state) {
            return [[0]]
          }
          let actionRewards = function (state, action) {
            let reward = {
              'bad': -1,
              'good': 1
            }[action]
            return [[reward]]
          }
          return {
            actionRewards,
            stateRewards,
            quantity: 1
          }
        }()

        /** No mental rewards for simplicity */
        let mentalStateDynamics = function () {
          return {
            estimationHeuristicArr: [],
            mentalStateArr: [],
            mentalUtilities: [[]]
          }
        }()

        let initialState = []

        let params = {
          // beliefRepresentation: 'dirichlet',
          numberOfAgents: 1,
          numberOfRewards: {
            physical: 1,
            mental: 0
          }
        }

        let rewardUtilityFunctions = {
          physical: [identity],
          mental: []
        }

        return {
          actions,
          transitionFn,
          initialState,
          physicalRewardStructure,
          mentalStateDynamics,
          rewardUtilityFunctions,
          API,
          params
        }
      }

      /** Define a basic scenario */
      let scenarios = function () {
        let agent = {
          params: {
            goalCoeffs: [1],
            metaParams: {
              alpha: 1000,
              discountFactor: 0.8,
              lookAhead: 2
            }
          },
          initialState: {
            belief: [null],
            mentalEstimations: [null],
            metaParamsEstimations: {
              alpha: [null],
              lookAhead: [null],
              discountFactor: [null]
            }
          }
        }

        let result = [
          {
            name: 'basic',
            agents: [ agent ],
            options: {
              horizon: 5,
              beliefRepresentation: 'dirichlet'
            }
          },
          {
            name: 'basic with discrete belief',
            agents: [ agent ],
            options: {
              horizon: 5,
              beliefRepresentation: 'discrete'
            }
          }
        ]

        return result
      }()

      let agents = ['bobby']

      let description = 'Another rather trivial test, but this time an' +
        ' agent has two available actions: *good* and *bad*. We expect he' +
        ' chooses *good* every time as he is (almost) perfectly rational'

      let validateOutput = function(trajectoriess) {
        assertIsArray(trajectoriess, OBJECT_TYPE, 2,
          "FAIL: trajectories array not as expected")
        map(function (trajectories) {
          const trajectory = trajectories[0]
          assertIsArray(trajectory, ARRAY_TYPE, 6,
            "FAIL: trajectory: " + toString(trajectory) +
              " expected to consist of five elements (arrays), but is not")
          mapIndexed(function (i, elem) {
            let state = repeat(i, function () {
              return 'good'
            })
            assertEqual(elem, [state, 'good'],
              'computed action not as expected')
          }, trajectories.slice(0, trajectories.length - 1))
        }, trajectoriess)
      }

      return {
        makeGame,
        scenarios,
        agents,
        description,
        validateOutput
      }
    }(),

    /**
     * THIRD TEST
     * one-player game but this time with mental rewards. In each state,
     * the agent can be __nice__ or __mean__. Being mean brings positive
     * physical rewards (+1 vs -1) but leads to low mental state (-1 vs +1).
     */
    function() {
      let makeGame = function () {
        let actions = function (state) { return ['nice', 'mean'] }

        let transitionFn = function (state, action) {
          let nextState = [action].concat(state)
          return Delta({v: nextState})
        }

        let API = function () {
          let getPreviousState = function (state) {
            return state.slice(1)
          }
          let getLastAction = function (state) {
            assert(!isInitial(state));
            return state[0]
          }
          let endsRound = function(state, action) {
            return true
          }
          let isInitial = function (state) {
            return state.length == 0
          }
          let turn = getConstantFn(0)

          let stateToString = arrayToString
          let API = {
            getPreviousState,
            getLastAction,
            endsRound,
            isInitial,
            turn,
            stateToString
          }
          return API
        }()

        let physicalRewardStructure = function () {
          let stateRewards = function (state) {
            return [[0]]
          }
          let actionRewards = function (state, action) {
            let reward = {
              'mean': 1,
              'nice': -1
            }[action]
            return [[reward]]
          }
          return {
            actionRewards,
            stateRewards
          }
        }()

        let mentalStateDynamics = function () {

          let computeMentalState = function(agentID, belief, state) {
            return {
              'mean': -1,
              'nice': 1
            }[state[0]] || 0
          }

          return {
            estimationHeuristicArr: [identity], /** not needed */
            mentalStateArr: [computeMentalState],
            mentalUtilities: [
              /** agent 0*/ [[0]]
            ]
          }
        }()

        let initialState = []

        let params = {
          beliefRepresentation: 'dirichlet',
          numberOfAgents: 1,
          numberOfRewards: {
            physical: 1,
            mental: 1 /** = sum (map length rewardTypeArr) */
          }
        }

        let rewardUtilityFunctions = {
          physical: [identity],
          mental: [identity]
        }

        return {
          actions,
          transitionFn,
          initialState,
          physicalRewardStructure,
          mentalStateDynamics,
          rewardUtilityFunctions,
          API,
          params
        }
      }

      /** Define a basic scenario */
      let scenarios = [
        {
          name: 'nice guy who cares about others',
          agents:
            [
              {
                params: {
                  /** format [physical goal, mental goal] */
                  goalCoeffs: [0.2,0.8],
                  metaParams: {
                    alpha: 1000,
                    discountFactor: 0.8,
                    lookAhead: 2
                  }
                },
                initialState: {
                  belief: [null],
                  mentalEstimations: [null],
                  metaParamsEstimations: {
                    alpha: [null],
                    lookAhead: [null],
                    discountFactor: [null]
                  }
                }
              }
            ],
          options: {
            horizon: 5,
            beliefRepresentation: 'dirichlet'
          }
        },
        {
          name: 'mean guy who cares about himself',
          agents:
            [
              {
                params: {
                  /** format [physical goal, mental goal] */
                  goalCoeffs: [0.9,0.1],
                  metaParams: {
                    alpha: 1000,
                    discountFactor: 0.8,
                    lookAhead: 2
                  }
                },
                initialState: {
                  belief: [null],
                  mentalEstimations: [null],
                  metaParamsEstimations: {
                    alpha: [null],
                    lookAhead: [null],
                    discountFactor: [null]
                  }
                }
              }
            ],
          options: {
            horizon: 5,
            beliefRepresentation: 'dirichlet'
          }
        }
      ]

      let agents = ['bobby']

      let description = 'Another one agent test, but this time, mental' +
        ' goal is included. Because there is only one agent, mental' +
        ' goal is agent\'s own state. We assume agent has two actions' +
        ' to choose from: \'nice\' and \'mean\' where the first one' +
        ' represents being nice to a person, resulting in lower' +
        ' physical rewards (as we\'re giving something up eg sharing)' +
        ' but higher mental rewards (feeling good about oneself).' +
        ' The second action is the opposite. We consider different' +
        ' scenarios modelling different types of agents.'

      let validateOutput = function(trajectoriess) {
        assertIsArray(trajectoriess, OBJECT_TYPE, 2,
          "FAIL: trajectories array not as expected: " +
          arrayToString(trajectoriess))
        let validateTrajectory = function(i, trajectories) {
          const trajectory = trajectories[0]
          assertIsArray(trajectory, OBJECT_TYPE, 6,
            "FAIL: trajectory: " + toString(trajectory) + " not as expected")
          let arr = ['nice', 'mean']
          mapIndexed(function(j, elem) {
            let state = repeat(j, function() { return arr[i]})
            assertEqual(elem, [state, arr[i]],
              'computed action not as expected')
          }, trajectory.slice(0, trajectory.length - 1))
        }
        mapIndexed(validateTrajectory, trajectoriess)
      }

      return {
        makeGame,
        scenarios,
        agents,
        description,
        validateOutput
      }
    }(),

    /**
     * FOURTH TEST
     * Model of tic tac toe - a two-player scenario, but no mental rewards for
     * now
     */
    function() {

      let makeGame = function () {

        /** state is called 'board' below */

        let isOver = function(board) {
          return threeInARow(board, 'X') ||
            threeInARow(board, 'O') ||
            all(isDefinedNotNull, board)
        }

        let indexBoard = function(board) {
          return mapIndexed(function(i, mark) {
            return [mark, i]
          }, board)
        }

        let marks = ['X', 'O']

        let getRows = function(board) {
          return map(function(i) {
            return board.slice(3*i, 3*i+3)
          }, rangeArray(0, 2))
        }

        let getCols = function(board) {
          let indexedBoard = indexBoard(board)
          map(function(colNo) {
            let indexedColumn = filter(function(indexedSquare) {
              return indexedSquare[1] % 3 === colNo
            }, indexedBoard)
            assertEqual(indexedColumn.length, 3,
              "getCols(board = " + stateToString(board) +
              "): wrong size of column")
            return map(function(indexedSquare) {
              return indexedSquare[0]
            }, indexedColumn)
          }, rangeArray(0,2))
        }

        let getDiagonals = function(board) {
          return [
            [board[0], board[4], board[8]],
            [board[2], board[4], board[6]]
          ]
        }

        let isWinning = function(line, mark) {
          return all(function(m) { return mark === m}, line)
        }

        let threeInARow = function(board, mark) {
          /** check rows */
          let rows = getRows(board)
          let cols = getCols(board)
          let diags = getDiagonals(board)
          let allLines = arrayConcat([rows, cols, diags])
          let winningLine = any(function (line) {
              return isWinning(line, mark)
            },
            allLines)
          return winningLine
        }

        /** actions are moves i.e. [mark, position] pairs */
        let actions = function (board) {
          if (isOver(board)) return ["nil"]
          let mark = marks[turn(board)]
          /** retrieve empty positions */
          let indexedSquares = indexBoard(board)
          let emptyIndexedSquares = filter(function(indexedSquare) {
            return isNull(indexedSquare[0])
          }, indexedSquares)
          let moves = map(function(emptyIndexedSquare) {
            return [mark, emptyIndexedSquare[1]]
          }, emptyIndexedSquares)
          return moves
        }

        let transitionFn = function (board, action) {
          if (action === "nil") return Delta({v: board})
          let mark = action[0]
          let pos = action[1]
          assert(isNull(board[pos]),
            "move " + arrayToString(action) + " in state " +
            stateToString(board) + " invalid!")
          assert(marks[turn(board)] === mark,
            "it's not turn of " + mark + " at state " +
            stateToString(board))
          let nextState = arrayReplace(board, pos, mark)
          return Delta({v: nextState})
        }


        /** represents board as follows:
         *  ___
         * |MMM|
         * |MMM|
         * |MMM|
         *
         * where each M is one of ' ', 'X', 'O'
         */
        let stateToString = function(board) {
          let short = true
          let firstLine = short ? "" : "\n ___"
          let result = reduceL(function(acc, mark) {
            let m = mark || " "
            let colNo = acc[0] % 3
            let append = short ? m : function() {
              if (colNo === 0) return "\n|" + m
              if (colNo === 2) return m + "|"
              return m
            }()
            return [acc[0]+1, acc[1] + append]
          }, [0, firstLine], board)
          return result[1] + (short ? "" : "\n")
        }

        let API = function () {
          let getPreviousState = function (state) {
            error("something's gone wrong: getPreviousState should not" +
              " be called as no mental and no belief update in this game")
          }
          let getLastAction = function (state) {
            error("something's gone wrong: getLastAction should not" +
              " be called as no mental and no belief update in this game")
          }
          let isInitial = function (state) {
            return all(isNull, state)
          }

          let turn = function(state) {
            let takenSquares = filter(isDefinedNotNull, state)
            return takenSquares.length % 2
          }

          let API = {
            getPreviousState,
            getLastAction,
            isInitial,
            turn,
            stateToString
          }
          return API
        }()

        let physicalRewardStructure = function () {
          let stateRewards = function (state) {
            if (threeInARow(state, 'X')) return [[1],[-1]]
            if (threeInARow(state, 'O')) return [[-1],[1]]
            return [[0],[0]]
          }
          let actionRewards = function (state, action) {
            return [[0], [0]]
          }
          return {
            actionRewards,
            stateRewards
          }
        }()

        /** No mental rewards for simplicity */
        let mentalStateDynamics = function () {

          return {
            estimationHeuristicArr: [], /** not needed */
            mentalStateArr: [],
            mentalUtilities: [
              [], /** agent 0*/
              []
            ]
          }
        }()

        let initialState = repeat(9, getNull)

        let params = {
          numberOfAgents: 2,
          numberOfRewards: {
            physical: 1,
            mental: 0 /** = sum (map length rewardTypeArr) */
          }
        }

        let rewardUtilityFunctions = {
          physical: [identity],
          mental: []
        }

        return {
          actions,
          transitionFn,
          initialState,
          physicalRewardStructure,
          mentalStateDynamics,
          rewardUtilityFunctions,
          API,
          params
        }
      }

      /** Define a basic scenario */
      let scenarios = function() {
        let params = {
          /** format [physical goal, mental goal] */
          goalCoeffs: [1],
            metaParams: {
              alpha: 1000,
              discountFactor: 0.9,
              lookAhead: 5
          }
        }
        let alpha = [null, Delta({v: 1000})]
        let discountFactor = [null, Delta({v: 0.9})]
        let lookAhead = [null, Delta({v: 2})]
        let mentalEstimations = [null, null]
        let belief = [
          null,
          Delta({v:[1]})
        ]
        let crossAgent = {
          params,
          initialState: {
            belief,
            mentalEstimations,
            metaParamsEstimations: {
              alpha, lookAhead, discountFactor
            }
          }
        }
        let noughtAgent = {
          params,
          initialState: {
            belief: arrayReverse(belief),
            mentalEstimations,
            metaParamsEstimations: {
              alpha: arrayReverse(alpha),
              lookAhead: arrayReverse(lookAhead),
              discountFactor: arrayReverse(discountFactor)
            }
          }
        }
        let result = [
          {
            name: 'can cross player find a winning move?',
            agents:
              [crossAgent, noughtAgent],
            options: {
              horizon: 1,
              beliefRepresentation: 'discrete'
            },
            startingState: [
              'X', null, 'X',
              'O', 'O', null,
              null, null, null
            ]
          },
          {
            name: 'can nought player find a winning move?',
            agents:
                [crossAgent, noughtAgent],
            options: {
              horizon: 1,
              beliefRepresentation: 'discrete'
            },
            startingState: [
                'X', 'O', 'X',
                'X', 'O', null,
                null, null, null
            ]
          },
          {
            name: 'can cross player find a winning tactic?',
            agents:
                [crossAgent, noughtAgent],
            options: {
              horizon: 5,
              beliefRepresentation: 'discrete'
            },
            startingState: [
              'X', 'O', null,
              null, null, null,
              null, null, null
            ]
          }
        ]
        return result
      }()

      let agents = ['alice','bob']

      let description = 'Tic-tac-toe is the first two player example' +
        ', it doesnt feature mental rewards so should be a little' +
        ' easier to deal with'

      let validateOutput = function(trajectoriess) {
        assertIsArray(trajectoriess, ARRAY_TYPE, scenarios.length,
          'FAIL: trajectoriess array should have ' + scenarios.length +
            ' elements (equal to number of scenarios), each an array, ' +
            'but found: ' + toString(trajectoriess))
        /** validate first scenario */
        const crossWinTrajectory = trajectoriess[0][0]
        const action = crossWinTrajectory[0][1][1]
        assertEqual(action, 1, 'FAIL: cross player expected to ' +
            'select action 2 (top right square) to win the game in first ' +
            'scenario, but he selected ' + action)
        const rewards = crossWinTrajectory[1][1]
        assertEqual(rewards[0], 1, 'FAIL: cross ' +
            'player expected to be the winner in first scenario, but instead ' +
            'his reward is ' + rewards[0])

        /** validate second scenario */
        const noughtWinTrajectory = trajectoriess[1][0]
        const action2 = noughtWinTrajectory[0][1][1]
        assertEqual(action2, 7, 'FAIL: nought player expected to ' +
            'select action 7 (bottom square) to win the game in second ' +
            'scenario, but he selected ' + action)
        const rewards2 = noughtWinTrajectory[1][1]
        assertEqual(rewards2[1], 1, 'FAIL: nought ' +
            'player expected to be the winner in second scenario, but instead ' +
            'his reward is ' + rewards2[1])

        /** validate third scenario */
        const crossWinningTacticTrajectory = trajectoriess[2][0]
        const rewards3 = crossWinningTacticTrajectory[5][1]
        assertEqual(rewards3[0], 1, 'FAIL: cross ' +
            'player expected to find the winning tactic in the second scenario,' +
            ' but instead his reward is ' + rewards3[0])
        // check also that nought has defended until the end
        assert(crossWinningTacticTrajectory[4][1][1] !== 'nil',
            'FAIL: nought player was expected to defend against threats' +
            ' and force cross player to win in 5 moves, no less; but cross won ' +
            'is less than five moves as his last action was nil')
      }

      return {
        makeGame,
        scenarios,
        agents,
        description,
        validateOutput
      }
    }(),

    /** FIFTH TEST (simple trust game with fixed params) */
    function() {
      let makeGame = function () {
        /** player 0 is alice (aka investor)
         *  player 1 is bob (aka investee) */
        let endowments = [2,1]
        let investorEndowment = endowments[0]
        let k = 2

        /** Auxiliary functions */
        let endowment = function(state) {
          if (turn(state) == 0) return endowments
          return [0,0]
        }

        let transfer = function(state, action) {
          let investor = action * ((turn(state) == 0) ? -1 : 1)
          let investee = (-1) * investor
          return [investor, investee]
        }

        let maxPossibleTransfer = function(state) {
          return (turn(state) === 0) ? investorEndowment :
            state.investments[0] * k
        }

        /** game mechanics */
        let actions = function (state) {
          if (turn(state) === 0) return rangeArray(0, investorEndowment)
          assert(state.investments.length > 0,
            "investee turn but no past investments recorded")
          return rangeArray(0, state.investments[0] * k)
        }

        let transitionFn = function(state, action) {
          let turn = turn(state)
          let nextTurn = (turn + 1) % 2
          let nextInvestments = (turn === 0) ?
            [action].concat(state.investments) : state.investments
          let nextReturns = (turn === 1) ?
            [action].concat(state.returns) : state.returns
          let result = {
            turn: nextTurn,
            investments: nextInvestments,
            returns: nextReturns
          }
          info("transitionFn(state: " + stateToString(state) + ", action: " + action + ")"
            + ": computed " + stateToString(result))
          return Delta({v: result})
        }

        let getPreviousState = function (state) {
          if (isInitial(state)) return state
          let prevTurn = (turn(state) + 1) % 2
          let prevInvestments = (prevTurn === 0) ? state.investments.slice(1) : state.investments
          let prevReturns = (prevTurn === 1) ? state.returns.slice(1) : state.returns
          return {
            turn: prevTurn,
            investments: prevInvestments,
            returns: prevReturns
          }
        }
        let getLastAction = function (state) {
          assert(!isInitial(state), "Calling previousAction on initial state")
          if (state.investments.length > state.returns.length)
            return state.investments[0]
          return state.returns[0]
        }

        let API = function () {
          let endsRound = function(state, action) {
            return state.investments.length > state.returns.length
          }
          let isInitial = function (state) {
            return arrayIsEmpty(state.investments) && arrayIsEmpty(state.returns)
          }
          let turn = function (state) {
            return state.turn
          }

          let stateToString = function (state) {
            return "{invs: " + arrayToString(state.investments) +
              ", rets: " + arrayToString(state.returns) + "}"
          }
          let API = {
            getPreviousState,
            getLastAction,
            endsRound,
            isInitial,
            turn,
            stateToString
          }
          return API
        }()

        let physicalRewardStructure = function () {
          let stateRewards = function(state) {
            let endowments = endowment(state)
            return [ [endowments[0]], [endowments[1]] ]
          }

          let actionRewards = function(state, action) {
            let transfers = transfer(state, action)
            return [ [transfers[0]], [transfers[1]] ]
          }

          return {
            actionRewards,
            stateRewards
          }
        }()


        let mentalStateDynamics = function () {

          let updateTrustEstimation = function(trust, estimatingAgentID, estimatedAgentID, state) {
            let lastState = getPreviousState(state)
            let action = getLastAction(state)
            let maxPossibleTransfer = maxPossibleTransfer(lastState)
            // trust is only updated if other agent took action
            if (maxPossibleTransfer == 0 || state.turn == estimatingAgentID) return trust
            let transferRatio = action / maxPossibleTransfer
            if (transferRatio > trust) {
              let trustIncreaseRatio = (transferRatio - trust) / (1 - trust)
              let lowRange = Math.log(trust) + 1
              let x = lowRange * (1 - trustIncreaseRatio) + trust * trustIncreaseRatio
              let result = Math.exp(x-1)
              assert(!_.isNaN(result) && result != undefined,
                "updateTrustValue returns " + result)
              return result
            } else {
              let trustDecreaseRatio = (trust - transferRatio) / trust
              let hiRange = Math.exp(trust) - 1
              let x = hiRange * (1 - trustDecreaseRatio) + trust * trustDecreaseRatio
              let result = Math.log(x+1)
              assert(!_.isNaN(result) && result != undefined,
                "updateTrustValue returns " + result)
              return result
            }
          }

          let computeTrust = function(agentID, belief, state) {
            let opponentID = otherAgentID(agentID)
            let individualBelief = retrieveBeliefOver(opponentID, belief)
            return goalCoeffExpectation(individualBelief, 1)
          }

          return {
            estimationHeuristicArr: [updateTrustEstimation], /** not needed */
            mentalStateArr: [computeTrust],
            mentalUtilities: [
              [[1]], /** agent 0 */
              [[0]]  /** agent 1 */
            ]
          }
        }()

        let initialState = {
          turn: 0,
          investments: [],
          returns: []
        }

        let params = {
          numberOfAgents: 2,
          numberOfRewards: {
            physical: 1,
            mental: 1 /** = length (mentalStateArr)
                          = length (estimationHeuristicArr) */
          }
        }

        let rewardUtilityFunctions = function() {
          let moneyUtility = function(x) {
            return x
          }

          let trustUtility = function(x) {
            return 15 * x
          }

          return {
            physical: [moneyUtility],
            mental: [trustUtility]
          }
        }()

        return {
          actions,
          transitionFn,
          initialState,
          physicalRewardStructure,
          mentalStateDynamics,
          rewardUtilityFunctions,
          API,
          params
        }
      }

      let scenarios = function() {
        let goalCoeffs = [0.2, 0.8]
        let metaParams = {
          alpha: 100,
          discountFactor: 0.8,
          lookAhead: 2
        }
        let params = {
          /** format [physical goal, mental goal] */
          goalCoeffs,
          metaParams
        }
        let alpha = [
          null,
          Categorical({vs: [100]})
        ]
        let lookAhead = [
          null,
          Categorical({vs: [2]})
        ]
        let discountFactor = [
          null,
          Categorical({vs: [0.8]})
        ]
        let metaParamsEstimations = {
          alpha,
          lookAhead,
          discountFactor
        }
        let mentalEstimations = [
            null,
            [Delta({v: 0.7})]
          ]
        let belief = [
          null, /** over agent 0 */
          [1,3] /** over agent 1 (investee) */
        ]
        let alice = {
          params,
          initialState: {
            belief,
            mentalEstimations,
            metaParamsEstimations
          }
        }
        let bob = {
          params,
          initialState: {
            belief: arrayReverse(belief),
            mentalEstimations: arrayReverse(mentalEstimations),
            metaParamsEstimations: {
              alpha: arrayReverse(alpha),
              lookAhead: arrayReverse(lookAhead),
              discountFactor: arrayReverse(discountFactor)
            }
          }
        }
        let result = [
          {
            name: 'basic: high trustworhty, high trust, perfect info',
            agents:
              [ alice, bob ],
            options: {
              horizon: 5,
              beliefRepresentation: 'dirichlet'
            }
          }
        ]
        return result
      }()

      let agents = ['alice', 'bob']

      let description = 'This is a basic trust game'

      let validateOutput = function(trajectories) {
        assertIsArray(trajectories, OBJECT_TYPE, 1,
          "FAIL: trajectories array not as expected: " +
          arrayToString(trajectories))
        // let validateTrajectory = function(i, trajectory) {
        //   assertIsArray(trajectory, OBJECT_TYPE, 5,
        //     "FAIL: trajectory: " + trajectory + "not as expected")
        //   let arr = ['nice', 'mean']
        //   mapIndexed(function(j, elem) {
        //     let state = repeat(j, function() { return arr[i]})
        //     assertEqual(elem, [state, arr[i]],
        //       'computed action not as expected')
        //   }, trajectory)
        // }
        // mapIndexed(validateTrajectory, trajectories)
      }

      return {
        makeGame,
        scenarios,
        agents,
        description,
        validateOutput
      }
    }()

  ]

  let runSingle = function(i) {
    assert(i >= 0 && i < testsArray.length,
      "index " + i + " of test invalid")
    let test = testsArray[i]
    display(emphasiseString("Test " + (i+1)))
    display(test.description)
    let trajectories =
      simulateScenarios(test.scenarios, test.makeGame, test.agents, test.callbacks)
    let validate = test.validateOutput
    validate(trajectories)
    display("PASS: Test " + (i+1))
  }

  let runAll = function() {
    map(runSingle, rangeArray(0, testsArray.length-1))
    return "all passed, congrats"
  }

  return { runAll, runSingle, quantity: testsArray.length }
}()

let runAll = tests.runAll
let runSingle = tests.runSingle
let quantity = tests.quantity

const testNo = argv.test
if (testNo >= 0) {
  assertHasType(testNo, INT_TYPE,
      'integration.wppl: --test argument must be an integer, ' +
      'found: ' + testNo + ' of type ' + typeof testNo)
  assertBetween(testNo, 0, quantity - 1,
      'integration.wppl: --test argument must identify an existing ' +
      'test, i.e., be between 0 and ' + (quantity - 1) + ', but found: ' + testNo)
  runSingle(testNo)
} else {
  runAll()
}
