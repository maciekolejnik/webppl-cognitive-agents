/** To run
 *  $ webppl examples/tictactoe/src/simulations.wppl --require . --require examples/tictactoe/ [--] [--help] [--experiment <experimentID>] [--scenario <scenarioID>] [--runs <numberOfRunsOfEachScenario>]
 */

/** Tic-tac-toe experiments
 *  They are divided into two parts:
 *  (1) non-cognitive, where mental goals don't feature in agents
 *  utility functions and the sole motivation is to win the game
 *  (2) cognitive, where the scenario is parent vs kid and kid's
 *  satisfaction is considered by the parent
 */

/** AUXILIARY */
/** prints the outcome of the game (should be called at the end) */
let final = function(game, agents, finalState, names) {
  let finalBoard = (!_top.Array.isArray(finalState[0])) ?
    finalState :
    function() {
      //convert into board
      let emptyBoard = repeat(9, getNull)
      return reduceL(function(board,move) {
        return arrayReplace(board, move[1], move[0])
      }, emptyBoard, finalState)
    }()
  if (threeInARow(finalBoard, 'X')) {
    display("winner: X")
  } else if (threeInARow(finalBoard, 'O')) {
    display("winner: O")
  } else {
    display("draw")
  }
}

/** PART 1: Non-cognitive Tic-tac-toe
 *  These experiments aim to show that agents with better cognitive
 *  abilities (measured by lookahead parameter) perform better in
 *  tic-tac-toe and also that agent's behaviour depends on their
 *  beliefs of opponent's lookahead
 * */
let lookAheadExperiment = function() {
  let getParams = function(lookAhead) {
    return {
      /** format [physical goal, mental goal] */
      goalCoeffs: [1],
      metaParams: {
        alpha: 1000,
        discountFactor: 0.9,
        lookAhead: lookAhead
      }
    }
  }
  let getInitialState = function(agentID, lookAheadEstimation) {
    let alpha = [null, Delta({v: 1000})]
    let discountFactor = [null, Delta({v: 0.9})]
    let lookAhead = [null, Delta({v: lookAheadEstimation})]
    let belief = [
      null,
      Delta({v:[1]})
    ]
    return {
      belief: (agentID == 0) ?
        belief : arrayReverse(belief),
      mentalEstimations : [null, null],
      metaParamsEstimations: {
        alpha: (agentID == 0) ?
          alpha : arrayReverse(alpha),
        lookAhead: (agentID == 0) ?
          lookAhead : arrayReverse(lookAhead),
        discountFactor: (agentID == 0) ?
          discountFactor : arrayReverse(discountFactor)
      }
    }
  }
  let getAgent = function(agentID, lookAhead, lookAheadEstimation) {
    return {
      params: getParams(lookAhead),
      initialState: getInitialState(agentID, lookAheadEstimation)
    }
  }

  let getScenarios = function() {
    return [
      {
        name: 'strong vs strong and both think their opponent strong',
        agents:
          [
            getAgent(0, 5, 4),
            getAgent(1, 5, 4)
          ],
        options: {
          horizon: 9,
          beliefRepresentation: 'discrete'
        },
        gameSpecificParams: {
          stateRepresentation: 'efficient'
        }
      },
      {
        name: 'strong vs weak but strong thinks opponent strong',
        agents:
          [
            getAgent(0, 5, 5),
            getAgent(1, 3, 3)
          ],
        options: {
          horizon: 9,
          beliefRepresentation: 'discrete'
        },
        gameSpecificParams: {
          stateRepresentation: 'efficient'
        }
      },
      {
        name: 'strong vs weak and strong thinks opponent weak',
        agents:
          [
            getAgent(0, 5, 3),
            getAgent(1, 3, 3)
          ],
        options: {
          horizon: 9,
          beliefRepresentation: 'discrete'
        },
        gameSpecificParams: {
          stateRepresentation: 'efficient'
        }
      },
      {
        name: 'weak vs weak and accurate estimations',
        agents:
          [
            getAgent(0, 3, 3),
            getAgent(1, 3, 3)
          ],
        options: {
          horizon: 9,
          beliefRepresentation: 'discrete'
        },
        gameSpecificParams: {
          stateRepresentation: 'efficient'
        }
      }
    ]
  }

  let callbacks = {
    final
  }

  let names = ['cross','nought']

  let runsDefault = 10

  // let runAll = function(repsOpt) {
  //   let reps = repsOpt || runsDefault
  //   simulateScenarios(scenarios, makeTicTacToe, names, callbacks, reps)
  // }
  //
  // let runE = function(experimentNumber, repsOpt) {
  //   assertHasType(experimentNumber, INT_TYPE,
  //     "lookAheadExperiment: run(): experimentNumber must be an " +
  //     "index into an array; found: " + toString(experimentNumber))
  //   assert(experimentNumber >= 0 && experimentNumber < scenarios.length,
  //     "lookAheadExperiment: run(): experimentNumber out of range: " +
  //     experimentNumber + "; expected between 0 and " + (scenarios.length - 1))
  //   let reps = repsOpt || runsDefault
  //   let scenario = scenarios.slice(experimentNumber, experimentNumber + 1)
  //   simulateScenarios(scenario, makeTicTacToe, names, callbacks, reps )
  // }

  return {
    name: 'lookAheadExperiment',
    desc: "Vary lookahead and lookahead estimation of agents to see how " +
      "that affects game outcomes",
    runsDefault: 10,
    // run,
    // runAll,
    getScenarios,
    names,
    callbacks
  }
}()


/**
 * PART 2: Tic-tac-toe with mental component (parent vs kid)
 */
let parentKidExperiment = function() {
  let getParentParams = function(firstCoeff, lookAhead) {
    return {
      /** format [physical goal, mental goal] */
      goalCoeffs: [firstCoeff, 1 - firstCoeff],
      metaParams: {
        alpha: 100,
        discountFactor: 1,
        lookAhead: lookAhead
      }
    }
  }
  let getParentInitialState = function(lookAheadEstimation) {
    let alpha = [Delta({v: 5}), null]
    let discountFactor = [Delta({v: 0.7}), null]
    let lookAhead = [Delta({v: lookAheadEstimation}), null]
    let belief = [
      Delta({v:[1]}),
      null
    ]
    return {
      belief,
      mentalEstimations :
        [
          [Delta({v: 0})], /** over agent 0 (kid) */
          null /** over oneself */
        ],
      metaParamsEstimations: {
        alpha,
        lookAhead,
        discountFactor
      }
    }
  }
  let getChildParams = function(alpha, discountFactor, lookAhead) {
    return {
      /** format [physical goal, mental goal] */
      goalCoeffs: [1],
      metaParams: {
        alpha,
        discountFactor,
        lookAhead
      }
    }
  }
  let getChildInitialState = function(lookAheadEstimation) {
    let alpha = [null, Delta({v: 100})]
    let discountFactor = [null, Delta({v: 0.9})]
    let lookAhead = [null, Delta({v: lookAheadEstimation})]
    let belief = [
      null,
      Delta({v:[1,0]})
    ]
    return {
      belief,
      mentalEstimations : [null, null],
      metaParamsEstimations: {
        alpha,
        lookAhead,
        discountFactor
      }
    }
  }
  let getChild = function(alpha, discountFactor, lookAhead, lookAheadEstimation) {
    return {
      params: getChildParams(alpha, discountFactor, lookAhead),
      initialState: getChildInitialState(lookAheadEstimation)
    }
  }
  let getParent = function(firstCoeff, lookAhead, lookAheadEstimation) {
    return {
      params: getParentParams(firstCoeff, lookAhead),
      initialState: getParentInitialState(lookAheadEstimation)
    }
  }

  let names = ['kid','parent']
  let startingState = [['X',0],['O',8],['X',4]]
  let startingState1 = [['X',2],['O',8],['X',4],['O',3],['X',0]]
  let startingState2 = [['O',7],['X',0],['O',8],['X',4]]
  let startingState3 = [['X',6],['O',7],['X',0],['O',8],['X',4]]
  let getScenarios = function() {
    return [
      {
        name: '(almost) perfectly rational kid with shortish lookahead (2)',
        agents:
          [
            getChild(100, 0.9, 2, 2),
            getParent(0.1, 3, 2)
          ],
        options: {
          horizon: 9,
          beliefRepresentation: 'discrete'
        },
        // startingState: startingState1,
        gameSpecificParams: {
          stateRepresentation: 'history'
        }
      },
      {
        name: 'params from qest paper: child rationality = 5, discountFactor' +
          ' = 0.7',
        agents:
          [
            getChild(5, 0.7, 2, 2),
            getParent(0.1, 3, 2)
          ],
        options: {
          horizon: 1,
          beliefRepresentation: 'discrete'
        },
        startingState: [['X', 0], ['O', 8], ['X', 4]],
        gameSpecificParams: {
          stateRepresentation: 'history'
        }
      }
    ]
  }

  let callbacks = {
    final
  }

  let run = function(experimentNumber) {
    assertHasType(experimentNumber, INT_TYPE,
      "lookAheadExperiment: run(): experimentNumber must be an " +
      "index into an array; found: " + toString(experimentNumber))
    assert(experimentNumber >= 0 && experimentNumber < scenarios.length,
      "lookAheadExperiment: run(): experimentNumber out of range: " +
      experimentNumber + "; expected between 0 and " + (scenarios.length - 1))
    let scenario = scenarios.slice(experimentNumber, experimentNumber + 1)
    simulateScenarios(scenario, makeTicTacToe, names, callbacks)
  }

  let runAll = function() {
    simulateScenarios(scenarios, makeTicTacToe, names, callbacks)
  }

  return {
    name: 'parentKidExperiment',
    desc: "Parent plays against a kid",
    getScenarios,
    names,
    callbacks
  }
}()

let experiments = [lookAheadExperiment, parentKidExperiment]

processCommandline(makeTicTacToe, experiments, argv)
