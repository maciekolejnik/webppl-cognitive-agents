/** Tipping game
 *  This scenario models cognitive process that underlies humans
 *  giving tips. The idea is that deciding on the tip amount involves
 *  a tradeoff between money and guilt. Tipping less saves one ('tipper')
 *  money but increases the feeling of guilt resulting from disappointing
 *  expectation of the 'tipee'.
 *
 *  Therefore, the utility function will consist of one physical reward
 *  (money) and one mental reward (guilt). We expect guilt to be
 *  inversely proportional to the tip amount, but also dependent on
 *  the quality of service received and cultural norms. Eg. tipping
 *  an averagely performing waiter in America 15% might be considered
 *  low and lead to high degree of guilt, while in some parts of Europe
 *  a 10% tip would be appropriate when excellent service was received
 *  and result in no guilt. Those norms are captured by a parameter of
 *  our model, *tippingNorm*, which takes a percentage value, usually
 *  between 0% and 20%. Each individual also has a unique proneness
 *  to guilt, which we represent via a (latent) parameter, gaspScore, of
 *  each agent.
 *
 *  We consider a simple one-shot scenario, with two agents that we refer
 *  to as 'tipper' (person giving the tip, aka player 1) and 'tipee'
 *  (person providing some service and receiving a tip, aka player 0).
 *  We assume that tipee moves first and their action captures the
 *  quality of service they provided (be it waiting, taxi ride,
 *  hotel consierge etc). For simplicity,
 *  we consider three different actions/QoS - bad, normal, good.
 *  Following receiving the service, we allow tips with various
 *  amounts: 0%, 5%, 10%, 15%, 20% and 25%. This interaction
 *  may then repeat as needed.
 *
 *  State is represented as usual by capturing the history of
 *  play so far, in particular an object of the form
 *  {
 *      pastService: <array of QoS action, most recent at index 0>,
 *      pastTips: <array of tip amounts, most recent at index 0>
 *  }
 *
 *  Utility functions differ between agents: the tipee only cares
 *  about the money, but tipper's utility depends on how much
 *  guilt they experience
 */

/**
 gameSpecificParams = {
    tippingNorm: <percentage value eg 5%,10%,20%>,
    gaspScores: <array giving GASP (0-7) for each agent, eg [4.57, 5.89]>
}
 */
let makeTippingCSMG = function(gameSpecificParams) {
  /** retrieve params for easier access.
   * make sure they're in the expected format */
  assertDefined(gameSpecificParams,
    "makeTippingCSMG(): gameSpecificParams undefined!")
  let tippingNorm = assertDefined(gameSpecificParams.tippingNorm,
  "makeTippingCSMG(): tippingNorm undefined")
  let gaspScores = assertDefined(gameSpecificParams.gaspScores,
    "makeTippingCSMG(): gaspScores undefined")

  assertHasType(tippingNorm, INT_TYPE, "tipping norm must be passed and be an integer")
  assertIsArray(gaspScores, NUMBER_TYPE, 2, "gaspScores array must be passed with two scores")

  /**
   * Game-specific auxiliary functions, constants etc
   */

  let TIPEE = 0
  let TIPPER = 1


  /**
   * Now come functions that describe the mechanics of the game
   */

  /** Actions available to an agent (the owner of *state*
   * - assumed to be unique) in *state* */
  let actions = function(state) {
    let actionsByPlayer = [['bad', 'normal', 'good'], [0,5,10,15,20,25,30]]
    return actionsByPlayer[turn(state)]
  }

  let transitionFn = function(state, action) {
    assert(actions(state).includes(action), "transitionFn(): "
      + action + " not allowed in state " + stateToString(state) +
    ". Might be a type mismatch")
    let nextState =
      (turn(state) == TIPEE) ?
        extend(state, { pastService: [action].concat(state.pastService) }) :
        extend(state, { pastTips: [action].concat(state.pastTips) })
    return Delta({v: nextState})
  }

  /**
   * Now comes the "API" of the game.
   * This is a set of functions which are used by our library but
   * whose implementation is game-specific.
   * */

  /** returns whose turn it is to take an action at *state*.
   *  this is also referred to as the owner of that state.
   *  players are identified by nonnegative integer */
  let turn = function (state) {
    info("turn() at state:")
    info(state)
    if (state.pastService.length > state.pastTips.length) {
      /** It's tipper's turn */
      return 1
    } else {
      return 0
    }
  }

  /** return action that was taken to get to *state* */
  let getLastAction = function (state) {
    assert(!isInitial(state), "Calling previousAction on initial state")
    return (turn(state) == TIPPER) ? state.pastService[0] : state.pastTips[0]
  }

  /** returns the string representation of *state*.
   * (for debugging purposes) */
  let stateToString = function (state) {
    assertDefined(state.pastService,
      "stateToString(): pastService undefined")
    assertDefined(state.pastTips,
      "stateToString(): pastTips undefined")
    return "{serviceHistory: " + arrayToString(state.pastService) +
      ",tipsHistory: " + arrayToString(state.pastTips) + "}"
  }

  let API = function() {
    /** return state which preceded *state* in the game, or *state* if it is initial state.
     * If called on initial state, this function can return whatever */
    let getPreviousState = function(state) {
      if (isInitial(state)) return state
      let lastTurn = otherAgentID(turn(state))
      let pastService = state.pastService
      let pastTips = state.pastTips
      let prevPastService = (lastTurn == TIPEE) ? pastService.slice(1) : pastService
      let prevPastTips = (lastTurn == TIPPER) ? pastTips.slice(1) : pastTips
      return {
        pastService: prevPastService,
        pastTips: prevPastTips
      }
    }

    /** checks whether *state* is the initial state */
    let isInitial = function (state) {
      return state.pastService.length == 0 && state.pastTips.length == 0
    }

    let actionSimilarity = function (state, a1, a2) {
      if (turn(state) === TIPEE) {
        if (_.isEqual(a1,a2)) return 0
        return -10
      }
      return -Math.abs(a1 - a2)
    }

    let API = {
      getPreviousState,
      getLastAction,
      isInitial,
      turn,
      stateToString,
      actionSimilarity
    }

    return API
  }()



  /**
   *  Now comes physical reward structure.
   *  This should typically be a simple component capturing what,
   *  and how much, physical rewards (money, time, number of sweets
   *  etc) agents receive in each state
   */
  let physicalRewardStructure = function() {

    /** Physical rewards gained by each agent at *state*
     * Should return an array indexed by agentID
     */
    let stateRewards = getConstantFn([[0],[0]])
    // let stateRewards = function(state) {
    //     return [[0], [0]]
    // }

    /** As above but for action rewards
     */
    let actionRewards = function(state, action) {
      if (turn(state) == TIPPER) return [[action], [-action]]
      return [[0],[0]]
    }

    return {
      actionRewards,
      stateRewards,
      quantity: 1
    }
  }()

  /**
   *  Now comes mental state dynamics model. That's the most important
   *  component, it captures the mental quantities (such as trust, guilt,
   *  pleasure, fairness, reciprocity).
   *  It consists of two components:
   *  (i) Heuristics agents use to estimate mental state of their opponents.
   *    This should be specified as an array of update functions, one for each
   *    mental state. Each update function has
   *    @type (mentalStateValue, estimatingAgent, estimatedAgent, state, action) -> newMentalStateValue
   *  (ii) Mental state computation, i.e. how can actual mental state of an
   *    agent be computed. Each such function that computes some mental state
   *    of an agent has
   *    @type (state, belief) -> mentalStateValue
   */
  let mentalStateDynamics = function() {
    /** We're concerned with the way agent perceives their own guilt
     *  so no need for estimating */
    let updateGuiltEstimation = function(guilt, estimatingAgent, estimatedAgent, state, observation) {
      return guilt
    }

    /** how does tipper "compute" guilt from tipping?
     *  note 'compute' is used informally as it would be
     *  more appropriate to say 'feel', 'perceive' etc.
     *  since our model operates with rewards rather than costs
     *  guilt will be negative (and the more guilty an agent
     *  feels, the lower its value will be, bounded by -1).
     *  We also allow positive values of guilt which reflect
     *  agent feeling good about themselves when giving
     *  'higher than expected' tip.
     *
     *  Guilt is assumed to be more or less logarithmic in
     *  the 'tip proportion', i.e. the relative size of the
     *  tip compared to what was expected.
     *  */
    let computeGuilt = function(agent, belief, state) {
      /** compute reference value, which specifies expected
       * tip given tipping norm and QoS received. */
      if (agent === TIPEE) return 0
      if (turn(state) === TIPPER) return 0
      if (isInitial(state)) return 0
      let ref = function() {
        let table = {
          'bad': tippingNorm * 0.4,
          'normal': tippingNorm,
          'good': tippingNorm * 1.3
        }
        return table[state.pastService[0]]
      }()
      assertHasType(ref, NUMBER_TYPE, "guilt reference value not a number: " + ref)
      assert(ref > 0, "tipping reference value must be >0")
      // let turn = turn(state)
      /** only tipper is guilty after their tip */
      let lastTip = getLastAction(state)
      assertHasType(lastTip, NUMBER_TYPE,
        "last action when computing guilt must be a tip, found: " + lastTip)
      /** guilt is modeled using log function appropriately
       * scaled according to the value of gasp score
       */
      // display("compute guilt at " + stateToString(state))
      let tipProp = (lastTip - ref) / ref
      let scale = (tipProp <= 0) ? 3/2 : 7
      let coeff = 1 - Math.exp(-scale)
      let gasp = gaspScores[TIPPER] / 7
      let guilt = Math.max(-1, Math.min(Math.log(tipProp * coeff + 1) / scale * gasp, 1))
      assertHasType(guilt, NUMBER_TYPE,
        "computed guilt should be a number, found: " + guilt)
      assert(guilt <= 1 && guilt >= -1,
        "guilt should be between -1 and 1, found: " + guilt)
      return guilt
    }

    return {
      estimationHeuristicArr: [ updateGuiltEstimation ],
      mentalStateArr: [ computeGuilt ],
      mentalUtilities: [
        [[]], /** agent 0 - tipee - cares about money */
        [[1]] /** agent 1 - tipper - feels guilty for tipping little */
      ]
    }
  }()

  /** Defines the initial state of the model (we assume there's only one such) */
  let initialState = {
    pastService: [],
    pastTips: []
  }

  /** belief representation and number of rewards (total = physical + mental)
   * must be specified */
  let params = {
    numberOfAgents: 2,
    numberOfRewards: {
      physical: 1,
      mental: 1
    }
  }

  /** Those are functions that make up the utility function that are
   * applied to each reward to possibly modify its value */
  let rewardUtilityFunctions = function() {
    let moneyUtility = function(x) {
      /** the constant 15 could be parameterised */
      let result = 2 / (1 + Math.exp(-x/15)) - 1
      // display("money utility of " + x + ": " + result)
      return result
    }

    // let guiltUtility = identity
    let guiltUtility = function(x) {
      // display("guilt=" + x)
      return 2*x
    }

    return {
      physical: [moneyUtility],
      mental: [guiltUtility]
    }
  }()

  return {
    actions,
    transitionFn,
    initialState,
    physicalRewardStructure,
    mentalStateDynamics,
    rewardUtilityFunctions,
    API,
    params
  }
}