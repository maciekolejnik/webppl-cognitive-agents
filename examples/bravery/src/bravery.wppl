/**
 * Bravery game
 * This is a simple one shot, two-player scenario in which only
 * player 1 is active. They have two actions to choose from: 'bold' and
 * 'timid'. The idea is that apart from their internal preference
 * for being timid, they are also being motivated by pleasing
 * their friends, modeled as player 2. To please their friends, player
 * 1 must estimate what they expect him/her to do and if they expect
 * bold, she/he's better of being bold, otherwise timid.
 *
 * On the other hand, friends (player 2) prefer their friend being
 * bold and moreover, they prefer to *think* of him as bold.
 *
 * This scenario is modeled using psychological games formalism and
 * three equilibria are found:
 * - bold
 * - timid
 * - 1/2 -> bold, 1/2 -> timid
 * Since beliefs are assumed accurate in equilibrium, strategy of
 * player 1 determines beliefs in equilibrium.
 *
 * We propose to model this scenario as a CSMG and offer insights
 * complementary to analysis via psychological games.
 * Our primary observation is that equilibrium behaviour may be
 * reached after repeated play, but is unlikely to arise from the
 * onset. Therefore, our model can be used to explain what happens
 * when those beliefs are inaccurate and when/if equilibrium is
 * reached.
 *
 * We model this scenario as follows:
 * Boldness of an agent will be modeled using goal coefficients.
 * In particular, there's one reward structure that assigns a unit
 * reward for being bold and another one that assigns a unit reward
 * for being timid. Adjusting goal coefficients corresponding to those
 * rewards makes an agent more bold or more timid.
 *
 * Additionally, we have a single mental reward structure that models
 * pride experienced by friends (player 2) following player 1's
 * decision. This pride is computed based on belief, detailed in
 * method *computePride* below. Now, player 1's motivation is dependent
 * on their second order belief, which is unsupported by our framework.
 * Instead, we assume that player 1 estimates pride of his/her friends
 * and gains their utility from that estimation.
 */


let makeBraveryGame = function(gameSpecificParams) {

  /** that regulates whether p2 prefer to think of their friend
   * as bold or timid. Ranges between -1 (strong preference for timid),
   * through 0 (no preference) to 1 (string preference for bold).
   * This value is reflected in computePride() below
   */
  let p2Bias = gameSpecificParams.bias

  let P1 = 0
  let P2 = 1
  /**
   * Now come functions that describe the mechanics of the game
   */

  /** Actions available to an agent (the owner of *state*
   * - assumed to be unique) in *state* */
  let actions = function(state) {
    if (turn(state) === P1)
      return ["bold", "timid"]
    if (_.isEqual(state.p1[0], "bold")) {
      return ["support", "suppress"]
    }
    return ["encourage", "support"]
  }

  /**  probabilistic transition function */
  let transitionFn = function(state, action) {
    if (turn(state) === P1) {
      let nextState = extend(state, {
        p1: [action].concat(state.p1)
      })
      return Delta({v: nextState})
    }
    let nextState = extend(state, {
      p2: [action].concat(state.p2)
    })
    return Delta({v: nextState})
  }

  let turn = function (state) {
    if (state.p1.length > state.p2.length) {
      return P2
    }
    return P1
  }

  /**
   * Now comes the "API" of the game.
   * This is a set of functions which are used by our library but
   * whose implementation is game-specific.
   * */
  let API = function() {
    let getPreviousState = function (state) {
      if (isInitial(state)) return state
      if (turn(state) === P1) {
        return extend(state, {
          p2: state.p2.slice(1)
        })
      }
      return extend(state, {
        p1: state.p1.slice(1)
      })
    }

    /** return action that was taken to get to *state* */
    let getLastAction = function (state) {
      assert(!isInitial(state), "Calling previousAction on initial state")
      if (turn(state) === P1) {
        return state.p2[0]
      }
      return state.p1[0]
    }

    let isInitial = function (state) {
      return arrayIsEmpty(state.p1) && arrayIsEmpty(state.p2)
    }

    /** returns the string representation of *state*.
     * (for debugging purposes) */
    let stateToString = function (state) {
      let actionsShortened = map(function(a) {
        return (_.isEqual(a, "timid") ? "T" : "B")
      }, state.p1)
      let reactionsShortened = map(function(a) {
        return (_.isEqual(a, "support") ? "=" :
          ((_.isEqual(a, "encourage")) ? "+" : "-"))
      }, state.p2)
      return "{ p1: " + toString(actionsShortened) + ", p2: " +
        toString(reactionsShortened) + "}"
      // return "{ p1: " + toString(state.p1) + ", p2: " +
      //   toString(state.p2) + "}"
    }

    /** does *action* taken in *state* end a round?
     * A concept of a round (which is an 'atomic' fragment of a game; it
     * either gets executed fully or not at all) is introduced so that
     * discounting can happen after every round, rather than after every move */
    let endsRound = function (state, action) {
      return turn(state) === P2
    }

    let API = {
      getPreviousState,
      getLastAction,
      isInitial,
      endsRound,
      turn,
      stateToString
    }

    return API
  }()

  /**
   *  Now comes physical reward structure.
   *  This should typically be a simple component capturing what,
   *  and how much, physical rewards (money, time, number of sweets
   *  etc) agents receive in each state
   */
  let physicalRewardStructure = function() {

    /** order: [bold, timid]
     */
    let stateRewards = function(state) {
      if (turn(state) === P2) {
        return (_.isEqual(state.p1[0], "bold")) ?
          [[1,0],[1,0]] : [[0,1],[0,1]]
      }
      return [[0,0], [0,0]]
    }

    /** As above but for action rewards
     */
    let actionRewards = function(state, action) {
      return [ [0,0], [0,0]]
    }

    return {
      actionRewards,
      stateRewards
    }
  }()

  /** mentalStateDynamics
   *
   * The mental variable we consider is pride. The idea is that player 2
   * (friends) are proud when they think of their friend as being brave
   * (captured by computePride).
   * Player 1 would like his friends to be proud of him, but he doesn't
   * know whether they have a preference for bold or timid. Hence,
   * player 1 estimates his friends' pride using hints provided by their
   * reactions to his actions (captured by updatePrideEstimation).
   *
   */
  let mentalStateDynamics = function() {

    let getLastAction = API.getLastAction
    let stateToString = API.stateToString

    /** An agent estimates that pride changes as a result of bold or
     * timid move based on history of feedback gained from player 2
     */
    let updatePrideEstimation =
      function(prevValue, estimatingAgentID, estimatedAgentID, state) {
        if (estimatedAgentID === P1) return 0
        if (turn(state) === P1) return prevValue
        // if (estimatingAgentID !== P1 || turn(state) !== P2)
        //   return 0
        let lastAction = getLastAction(state)
        assert(_.isEqual(lastAction, "bold") || _.isEqual(lastAction, "timid"),
          "updatePrideEstimation(): found lastAction=" +
          toString(lastAction) + "; expected 'bold' or 'timid'")
        let actionsPairs = zip(state.p1.slice(1), state.p2)
        let a = reduceL(function(acc, actionPair) {
          let curVal = acc[0]
          let factor = acc[1]
          let action = actionPair[0]
          let reaction = actionPair[1]
          let newFactor = Math.max(factor * 0.8, 0.05)
          if ((_.isEqual(action, "bold") && _.isEqual(reaction, "support")) ||
            (_.isEqual(action, "timid") && _.isEqual(reaction, "encourage"))) {
            return [Math.min(1, curVal + factor), newFactor]
          }
          return [Math.max(-1, curVal - factor), newFactor]
        }, [0, 0.5], actionsPairs)
        let timidBoldCoeff = a[0]
        assert(timidBoldCoeff >= -1 && timidBoldCoeff <= 1,
          "updatePrideEstimation(): computed timidBoldCoeff=" +
        timidBoldCoeff + "; expected in [-1,1]")
        // display("timidBoldCoeff=" + timidBoldCoeff + " at state: "
        //   + stateToString(state))
        let actionAsInt = {
          'bold': 1,
          'timid': -1
        }[lastAction]
        assert(actionAsInt === 1 || actionAsInt === -1,
          "updatePrideEstimation(): actionAsInt should be -1 " +
          "or 1; found: " + actionAsInt)
        // let iteration = 1 + state.p2.length
        // let scale = 1 / Math.exp(0.35 * iteration) * Math.abs(timidBoldCoeff)
        let scale = Math.abs(timidBoldCoeff)
        let prideEstimate = (Math.abs(actionAsInt - timidBoldCoeff) <= 1) ?
          prevValue + (1 - prevValue) * scale :
          // prevValue - (prevValue + 1) * scale
          -scale
        // let prideEstimate = (_.isEqual(lastAction, "bold") ?
        //   (1+timidBoldCoeff) / 2 : (1-timidBoldCoeff) / 2)
        assert(prideEstimate >= -1 && prideEstimate <= 1,
          "updatePrideEstimation(): computed prideEstimate=" +
          prideEstimate + "; expected in [-1,1]")
        // display("updatePrideEstimation(): pride estimate at state "
        //   + stateToString(state) + ": " + prideEstimate)
        return prideEstimate
      }

    let computePride = function(agentID, belief, state) {
      if (agentID === 0) return 0
      /** Pride experienced only immediately after p1's move */
      if (turn(state) === P1) return 0
      // let actionsPairs = zip(state.p1.slice(1), state.p2)
      // let goodFeedbackPairs = filter(function(pair) {
      //   return (_.isEqual(pair[0], "timid") && _.isEqual(pair[1], "encourage"))
      //   || (_.isEqual(pair[0], "bold") && _.isEqual(pair[1], "support"))
      // }, actionsPairs)
      // let proportion = (actionsPairs.length === 0) ?
      //   1 : goodFeedbackPairs.length / actionsPairs.length
      // assert(proportion >= 0 && proportion <= 1,
      //   "computePride(): proportion of good feedback pairs " +
      //   "expected in [0,1]; found: " + proportion)
      let individualBelief = retrieveBeliefOver(0, belief)
      let boldCoeff = goalCoeffExpectation(individualBelief, 0)
      let timidCoeff = goalCoeffExpectation(individualBelief, 1)
      let sum = boldCoeff + timidCoeff
      let boldness = boldCoeff / ((1 + sum)/2)
      let timidness = timidCoeff / ((1 + sum)/2)
      assert(boldness >= 0 && boldness <= 1,
        "computePride(): boldBelief expected in [0,1]; found: "
        + boldness)
      let getLastAction = API.getLastAction
      let lastAction = getLastAction(state)
      if (_.isEqual(lastAction, 'bold')) {
        return (p2Bias >= 0) ? boldness * p2Bias : - timidness/2 * p2Bias
      }
      return (p2Bias >= 0) ? -boldness/2 * p2Bias : - timidness * p2Bias
    }

    return {
      estimationHeuristicArr: [ updatePrideEstimation ],
      mentalStateArr: [ computePride ],
      mentalUtilities: [
        [[1]], /** agent 0 */
        [[1]] /** agent 1 */
      ]
    }
  }()

  let heuristics = function() {
    let action = function() {
      let applies = function(state, estimatingAgentID, estimatedAgentID) {
        return false
      }

      let computeOpponentAction =
        function(estimatingAgentID, estimatedAgentID, action,
                 state, getMentalState, getMentalEstimation) {
          return false
        }

      return {
        applies,
        computeOpponentAction
      }
    }()

    let belief = function() {
      /** does heuristic apply for updating belief of
       *agent* when *action* taken at state */
      let applies = function(state, action, agent) {
        return false
      }

      let updateBelief = function(fullBelief, state, action) {
        return fullBelief
      }

      return {
        applies,
        updateBelief
      }
    }()

    return {
      action,
      belief
    }
  }()

  let initialState = {
    p1: [],
    p2: []
  }

  let params = {
    numberOfAgents: 2,
    numberOfRewards: {
      physical: 2,
      mental: 1
    }
  }

  let rewardUtilityFunctions = function() {
    let timesTwo = function(x) {
      return 2*x
    }
    return {
      physical: [identity, identity],
      mental: [timesTwo]
    }
  }()

  return {
    actions,
    transitionFn,
    initialState,
    physicalRewardStructure,
    mentalStateDynamics,
    heuristics,
    rewardUtilityFunctions,
    API,
    params
  }
}