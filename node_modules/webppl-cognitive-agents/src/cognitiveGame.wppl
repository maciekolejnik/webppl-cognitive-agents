let getGameAPI = function (gameSpecificAPI) {

  let stateToString = gameSpecificAPI.stateToString

  let mentalSnapshotToString = function (mentalSnapshot) {
    let state = stateToString(mentalSnapshot.state)
    let values = mentalSnapshot.values
    return "{state: " + state + ", values: " + values + "}"
  }

  /** This implements default action similarity measure (which we could
   * call 'discrete measure') in case a measure is not provided in
   * user game specification */
  let actionSimilarity = function(state, a1, a2) {
    assertDefined(state, "actionSimilarity(): state undefined")
    assertDefined(a1, "actionSimilarity(): a1 undefined")
    assertDefined(a2, "actionSimilarity(): a2 undefined")
    let actionSimilarityOpt = gameSpecificAPI.actionSimilarity
    if (isDefined(actionSimilarityOpt)) return actionSimilarityOpt(state, a1,a2)
    if (_.isEqual(a1,a2)) return 0
    return -100
  }

  let APIExtension = {
    mentalSnapshotToString,
    actionSimilarity
  }

  let gameAPI = extend(gameSpecificAPI, APIExtension)
  return gameAPI
}


/** makeCSMG (cognitive stochastic multiplayer game)
 * @param gameSetup
 *  an object of the form
 *  {
    actions,
    transition,
    initialState,
    API,
    getPhysicalRewards,
    getMentalStateDynamics,
    utilityFn,
    params
    }
 * This object contains all the game specific information needed
 * to create a CSMG and it is defined by the user,
 * @param externalParams
 * of the form
 * {
 *   beliefRepresentation
 * }
 *
 * Given a game configuration, this generic function creates a
 * cognitive model. This mostly involves creating an appropriate
 * mental reward structure.
 */
let makeCSMG = function (gameSetup, externalParams, observationsOpt) {
  /** Extract basic components and validate user-defined *gameSetup* */
  assertDefined(gameSetup,
    "makeCSMG(): gameSetup undefined")
  assertDefined(externalParams,
    "makeCSMG(): externalParams undefined")
  let params = assertDefined(gameSetup.params,
    "makeCSMG(): gameSetup.params undefined")
  let beliefRepresentation = assertDefined(externalParams.beliefRepresentation,
    "makeCSMG(): params.beliefRepresentation undefined")
  let numberOfAgents = assertDefined(params.numberOfAgents,
    "makeCSMG(): params.numberOfAgents undefined")
  let numberOfRewards = assertDefined(params.numberOfRewards,
    "makeCSMG(): params.numberOfRewards undefined")
  assertDefined(numberOfRewards.physical,
    "makeCSMG(): numberOfRewards.physical undefined")
  assertDefined(numberOfRewards.mental,
    "makeCSMG(): numberOfRewards.mental undefined")

  let actions = assertDefined(gameSetup.actions,
    "makeCSMG(): gameSetup.actions undefined")
  let transitionFn = assertDefined(gameSetup.transitionFn,
    "makeCSMG(): gameSetup.transitionFn undefined")
  let initialState = assertDefined(gameSetup.initialState,
    "makeCSMG(): gameSetup.params undefined")

  /** Extract game-specific API calls for easy access */
  let API = getGameAPI(gameSetup.API)

  let getPreviousState = assertDefined(API.getPreviousState,
    "makeCSMG(): API.getPreviousState undefined")
  let getLastAction = assertDefined(API.getLastAction,
    "makeCSMG(): API.getLastAction undefined")
  let isInitial = assertDefined(API.isInitial,
    "makeCSMG(): API.isInitial undefined")
  let stateToString = assertDefined(API.stateToString,
    "makeCSMG(): API.stateToString undefined")

  /** Reward structures */
  let physicalRewardStructure = assertDefined(gameSetup.physicalRewardStructure,
    "makeCSMG(): gameSetup.physicalRewardStructure undefined")
  let mentalStateDynamics = assertDefined(gameSetup.mentalStateDynamics,
    "makeCSMG(): gameSetup.mentalStateDynamics undefined")
  let estimationHeuristicsArr = assertDefined(mentalStateDynamics.estimationHeuristicArr,
    "makeCSMG(): estimationHeuristicsArr undefined")
  let mentalStateArr = assertDefined(mentalStateDynamics.mentalStateArr,
    "makeCSMG(): mentalStateArr undefined")
  let mentalUtilities = assertDefined(mentalStateDynamics.mentalUtilities,
    "makeCSMG(): mentalUtilities undefined")
  assertIsArray(estimationHeuristicsArr, FUNCTION_TYPE, numberOfRewards.mental,
    "makeCSMG(): estimationHeuristicArr: " + arrayToString(estimationHeuristicsArr))
  assertIsArray(mentalStateArr, FUNCTION_TYPE, numberOfRewards.mental,
    "makeCSMG(): mentalStateArr: " + arrayToString(mentalStateArr))
  assertIsArray(mentalUtilities, ARRAY_TYPE, numberOfAgents,
    "makeCSMG(): mentalUtilities: " + arrayToString(mentalUtilities))

  /** different agents are motivated by different things so number of
   * goal coefficients generally differs between agents */
  let goalCoeffsNumberByAgent =
    computeGoalCoeffsNumber(numberOfRewards.physical, mentalUtilities)


  /** Reward Utilities */
  let rewardUtilityFunctions = assertDefined(gameSetup.rewardUtilityFunctions,
    "makeCSMG(): gameSetup.rewardUtilityFunctions undefined")
  assertDefined(rewardUtilityFunctions.physical, "makeCSMG():" +
    "gameSetup.rewardsUtilityFunctions.physical undefined")
  assertDefined(rewardUtilityFunctions.mental, "makeCSMG():" +
    "gameSetup.rewardsUtilityFunctions.mental undefined")
  assertEqual(rewardUtilityFunctions.physical.length, numberOfRewards.physical,
    "makeCSMG(): number of physical reward utility functions " +
    "expected to match number of physical rewards; found: " +
    rewardUtilityFunctions.physical.length + ", " + numberOfRewards.physical)
  assertEqual(rewardUtilityFunctions.mental.length, numberOfRewards.mental,
    "makeCSMG(): number of mental reward utility functions " +
    "expected to match number of mental rewards; found: " +
    rewardUtilityFunctions.mental.length + ", " + numberOfRewards.mental)

  /** Physical rewards structure
   *  This simply delegates to the physicalRewards object as given
   *  in gameSetup
   */
  let getPhysicalRewardStructure = function () {
    return physicalRewardStructure
  }

  /** Mental reward structure uses mental state dynamics provided in
   * @gameSetup to enable computation of mental rewards in any state.
   * Primarily, this is used by an agent to estimate mental state of
   * their opponent.
   * However, it is also used by an agent to estimate their own mental
   * state at some future point, thereby not requiring to compute belief
   * of that agent.
   *
   * @param initialEstimations (@type array of array of distributions,
   * with holes, indexed by agent)
   *  initial mental state estimations of this agent
   * @param selfAgentID
   *  identifier (nonnegative integer, usually 0 or 1) of an agent
   *  identifies the agent for which this reward structure is provided
   */
  let getMentalRewardStructure = function (initialEstimations, selfAgentID) {
    /**
     * @type array of update functions (each of @type (value, agent, state, action) -> newValue)
     */

    /** Represents @selfAgentID computing mental rewards of @ofAgentID.
     *
     * There are two cases:
     * - if @mentalSnapshot is missing, @agentID is computing its own mental rewards
     *   as part of selecting its best action (expected utility)
     * - if @mentalSnapshot is passed, @agentID is computing its opponent's mental
     *   rewards, either as part of selecting an action or updating belief
     * @param state
     * @param ofAgentID
     * @param belief
     * @param mentalSnapshot (optional)
     *  a reference point from which to use mental state dynamics
     *  it's an object { state, values }
     *  (it snapshots mental state values @estimations at @state)
     *  if omitted, initial state with initial estimations serves as snapshot
     *  snapshot is not used when this agent's mental rewards are being computed
     *
     *  @returns [<rewards for mental state 0>, <rewards for mental state 1>, ...]
     */
    let computeMentalRewards = function (state, ofAgentID, belief, mentalSnapshot) {
      info("computeMentalRewards(): " + selfAgentID + " computing mental " +
        "rewards of " + ofAgentID + " at state " + stateToString(state))

      // precompute mental state of selfAgentID; not strictly necessary,
      // as not all the values will be used
      // let mentalState = computeMentalState(state, belief)
      // info("computeMentalRewards(): mental state computed: " + arrayToString(mentalState))
      /** ofAgentMentalUtility is an array with one elem per mental attitude
       * and that elem is an array which identifies agents whose attitude
       * (this particular one) ofAgentID cares about */
      let ofAgentMentalUtility = mentalUtilities[ofAgentID]
      let computeRewardsFromAttitude = function(mentalAttitudeIndex, agentArr) {
        /** selfAgentID estimates mental reward gained by ofAgentID from
         * overAgentID's mental state  */
        let estimateReward = function(overAgentID) {
          if (selfAgentID === overAgentID) {
            if (selfAgentID === ofAgentID)
              return computeMentalState(state, belief, mentalAttitudeIndex)
              // return mentalState[mentalAttitudeIndex]
            else
              return nestedEstimation(state, mentalAttitudeIndex, mentalSnapshot)
          }
          return expectation(estimation(state, overAgentID, mentalAttitudeIndex))
        }
        return map(estimateReward, agentArr)
      }
      let rewardsByAttitudeArr = mapIndexed(computeRewardsFromAttitude, ofAgentMentalUtility)
      return rewardsByAttitudeArr


      // if (selfAgentID == ofAgentID) {
      //   info("computeMentalRewards(): " + selfAgentID + " computing their own rewards")
      //   /**
      //    * @param mentalAttitudeIndex | identifies mental state
      //    * @param agentArr | identifies agents whose mental state selfAgentD cares about
      //    */
      //   let computeRewardsFromAttitude = function (mentalAttitudeIndex, agentArr) {
      //     let estimateReward = function(overAgentID) {
      //       if (overAgentID === selfAgentID) return mentalState[mentalAttitudeIndex]
      //       return expectation(estimation(state, overAgentID, mentalAttitudeIndex))
      //     }
      //     return map(estimateReward, agentArr)
      //   }
      //   let rewardsByAttitudeArr = mapIndexed(computeRewardsFromAttitude, ofAgentMentalUtility)
      //   info("computeMentalRewards(): about to join " + arrayToString(rewardsByAttitudeArr))
      //   return rewardsByAttitudeArr
      // } else {
      //   /** now selfAgentID is computing (estimation) ofAgentID's mental rewards
      //    * this is nested reasoning and we simplify it by assuming that selfAgentID
      //    * uses
      //    * * his mental state estimations to estimate ofAgentID's own mental state
      //    * * estimation from snapshot to estimate ofAgentID's estimations of
      //    * selfAdgentID's mental state
      //    * * his own estimations to estimate ofAgentID's estimation of other
      //    * agents' mental state
      //    */
      //   let estimateRewardsFromAttitude = function (mentalAttitudeIndex, agentArr) {
      //     /** i'th mental attitude, agentArr identifies agents whose
      //      * mental state we care about */
      //     let estimateReward = function(overAgentID) {
      //       /** j identifies agent whose mental state we're trying to
      //        * estimate */
      //       if (overAgentID === selfAgentID) {
      //         return nestedEstimation(state, mentalAttitudeIndex, mentalSnapshot)
      //       }
      //       return expectation(estimation(state, overAgentID, mentalAttitudeIndex))
      //     }
      //     return map(estimateReward, agentArr)
      //   }
      //   let rewardsByAttitudeArr = mapIndexed(estimateRewardsFromAttitude, ofAgentMentalUtility)
      //   info("computeMentalRewards(): about to join " + arrayToString(rewardsByAttitudeArr))
      //   return rewardsByAttitudeArr
      // }
    }

    let computeMentalState = function(state, belief, mentalAttitudeIndex) {
      info("mentalState() of agent " + selfAgentID + " at state " +
        stateToString(state) + " for mental attitude " + mentalAttitudeIndex)
      let mentalStateComputeFn = mentalStateArr[mentalAttitudeIndex]
      let result = mentalStateComputeFn(selfAgentID, belief, state)
      info("mentalState(): computed " + result)
      return result
    }

    // /** returns array of values of mental states */
    // let computeMentalState = function (state, belief) {
    //   info("computeMentalState(): state=" + stateToString(state) +
    //     ", belief: " + beliefToString(belief, selfAgentID, goalCoeffsNumberByAgent))
    //   let f = function (mentalStateComputeFn) {
    //     info("call mentalStateComputeFn for agent " + selfAgentID)
    //     return mentalStateComputeFn(selfAgentID, belief, state)
    //   }
    //   let result = map(f, mentalStateArr)
    //   info("computeMentalState(): returning")
    //   return result
    // }

    /** returns the estimated value of this agent's mental state (identified
     * by *mentalAttitudeIndex*), computed using mental state dynamics
     * relative to a true value at some past state saved in *mentalSnapshot*
     * */
    let nestedEstimation = function(state, mentalAttitudeIndex, mentalSnapshot) {
      info("nestedEstimation(state=" + stateToString(state) + ", index=" +
        mentalAttitudeIndex + ")")
      assertDefined(state,
        "nestedEstimation(): state undefined!")
      assertDefined(mentalAttitudeIndex,
        "nestedEstimation(): mentalAttitudeIndex undefined!")
      assertDefined(mentalSnapshot,
        "nestedEstimation(): mentalSnapshot undefined!")
      assertDefined(mentalSnapshot.state,
        "nestedEstimation(): mentalSnapshot.state undefined!")
      assertDefined(mentalSnapshot.values,
        "nestedEstimation(): mentalSnapshot.values undefined!")
      let snapState = mentalSnapshot.state
      let snapValues = mentalSnapshot.values
      let updateFn = estimationHeuristicsArr[mentalAttitudeIndex]
      /** Local recursive function to save stack space by passing less parameters */
      let nestedEstimationRec = function(state) {
        if (_.isEqual(state, snapState)) {
          return snapValues[mentalAttitudeIndex]
        }
        let prevState = getPreviousState(state)
        // let lastAction = getLastAction(state)
        let prevValue = nestedEstimationRec(prevState)
        let curValue = updateFn(prevValue, selfAgentID, selfAgentID, state)
        return curValue
      }
      let result = nestedEstimationRec(state)
      info("nestedEstimation(): returning")
      return result
    }

    // let computeMentalRewardRecFromSnapshot = dp.cache(function (state, mentalSnapshot) {
    //   assertDefined(mentalSnapshot,
    //     "mentalRewardRecFromSnapshot: Mental snapshot undefined!")
    //   let snapState = mentalSnapshot.state
    //   let snapValues = mentalSnapshot.values
    //   if (_.isEqual(state, snapState)) {
    //     return snapValues
    //   }
    //   let prevState = getPreviousState(state)
    //   let lastAction = getLastAction(state)
    //   let prevValues = computeMentalRewardRecFromSnapshot(prevState, mentalSnapshot)
    //   let updateValue = function (updateFn, prevValue) {
    //     return updateFn(prevValue, selfAgentID, prevState, lastAction)
    //   }
    //   let curValues = map2(updateValue, estimationHeuristicsArr, prevValues)
    //   return curValues
    // })

    /** computes this agent's (selfAgentID) estimation of *ofAgentID*
     * mental state (attitude *rewardIndex*) */
    let estimation =
        dp.cache(function(state, ofAgentID, rewardIndex) {
        info("estimation(): state=" + stateToString(state) +
          ", ofAgentID=" + ofAgentID + ", rewardIndex=" + rewardIndex)
        if (isInitial(state)) {
          assert(initialEstimations[ofAgentID] !== undefined &&
            initialEstimations[ofAgentID][rewardIndex] !== undefined,
            "initial estimations of agent " + selfAgentID +
            " about " + ofAgentID + " on mental reward " + rewardIndex +
            " requested, but undefined")
          return initialEstimations[ofAgentID][rewardIndex]
        }
        let prevState = getPreviousState(state)
        let prevEstimation = estimation(prevState, ofAgentID, rewardIndex)
        let updateEstimation = function (updateFn, prevEstimation) {
          return Infer({method: 'enumerate'}, function () {
            assertHasType(prevEstimation, DIST_TYPE,
              "estimation(): " + selfAgentID + "'s initial " +
              "estimation of " + ofAgentID + "'s mental state " +
              rewardIndex + " is not a distribution: it's " +
              toString(prevEstimation))
            let prevValue = sample(prevEstimation)
            return updateFn(prevValue, selfAgentID, ofAgentID, state)
          })
        }
        // let curEstimations = map2(updateEstimation, estimationHeuristicsArr, prevEstimations)
        let result = updateEstimation(estimationHeuristicsArr[rewardIndex], prevEstimation)
        info("estimation(): state=" + stateToString(state) +
          "; returning " + result)
        return result
      })

    // let computeMentalStateEstimations = dp.cache(function (state, ofAgentID) {
    //   if (isInitial(state)) return initialEstimations[ofAgentID]
    //   let prevState = getPreviousState(state)
    //   let lastAction = getLastAction(state)
    //   let prevEstimations = computeMentalStateEstimations(prevState, ofAgentID)
    //   let updateEstimation = function (updateFn, prevEstimation) {
    //     return Infer({method: 'enumerate'}, function () {
    //       let prevValue = sample(prevEstimation)
    //       return updateFn(prevValue, other(selfAgentID), prevState, lastAction)
    //     })
    //   }
    //   let curEstimations = map2(updateEstimation, estimationHeuristicsArr, prevEstimations)
    //   return curEstimations
    // })

    return {
      computeMentalRewards,
      computeMentalState
      // mentalUtilities /** hacky, should not be here */
    }
  }

  /** Compute utility assuming only reward identified by index counts
   * @param  rewards {Object}   must specify physical and mental rewards
   * @param  index   {Integer}  identifies the reward, it may be physical
   *                            or mental
   * @return          {Number}  utility
   *  */
  // let utilityAtIndex = function(rewards, index) {
  //   info("utilityAtIndex(): rewards: " + arrayToString(rewards) +
  //   ", index: " + index)
  //   let physicalNo = rewards.physical.length
  //   if (index < physicalNo) {
  //     let rewardUtilityFn = rewardUtilityFunctions.physical[index]
  //     return rewardUtilityFn(rewards.physical[index])
  //   }
  //   let mentalIndex = index - physicalNo
  //   let indexes = arrayConcat(mapIndexed(function(idx, rews) {
  //     return repeat(rews.length, function() { return idx })
  //   }, rewards.mental))
  //   let rewardUtilityFn = rewardUtilityFunctions.mental[indexes[mentalIndex]]
  //   let mentalRewardsFlattened = arrayConcat(rewards.mental)
  //   let utility = rewardUtilityFn(mentalRewardsFlattened[mentalIndex])
  //   info("utilityAtIndex(): returning " + utility + " of reward " +
  //     mentalRewardsFlattened[mentalIndex])
  //   return utility
  // }

  /** Computes mental utility given a nested array of rewards
   * (mentalRewards) which is indexed by mentalAttitude
   *
   * @param goalCoeffs      {Array<Double>}
   * @param physicalRewards {Array<Double>}
   * @param indexOpt        {Int}            (optional)
   */
  let physicalUtilityFn = function(goalCoeffs, physicalRewards, indexOpt) {
    assertEqual(goalCoeffs.length, physicalRewards.length,
      "physicalUtilityFn(): goalCoeffs (" + toString(goalCoeffs)
      + ") dimensions don't match physicalRewards (" + toString(physicalRewards)
      + ") dimensions")
    assertEqual(goalCoeffs.length, rewardUtilityFunctions.physical.length,
      "physicalUtilityFn(): goalCoeffs (" + toString(goalCoeffs)
      + ") dimensions don't match rewardUtilityFunctions.physical " +
      "dimensions: " + rewardUtilityFunctions.physical.length)
    info("physicalUtilityFn(): goalCoeffs=" + toString(goalCoeffs) +
    ", physicalRewards=" + toString(physicalRewards) + ", indexOpt=" + indexOpt)
    if (isDefined(indexOpt)) {
      assertBetween(indexOpt, 0, goalCoeffs.length-1,
        "physicalUtilityFn(): index out of bounds: " + indexOpt +
      "; expected between 0 and " + (goalCoeffs.length - 1))
      return goalCoeffs[indexOpt] * apply(rewardUtilityFunctions.physical[indexOpt], physicalRewards[indexOpt])
    }
    let physicalRewardUtilities =
      map2(apply, rewardUtilityFunctions.physical, physicalRewards)
    let utility = sum(map2(multiply, goalCoeffs, physicalRewardUtilities))
    return utility
  }

  /** Computes mental utility given a nested array of rewards
   * (mentalRewards) which is indexed by mentalAttitude
   *
   * @param goalCoeffs {Array<Double>}
   * @param mentalRewards {Array<Double>}
   * @param indexOpt {Int} (optional) must be passed relative to mental rewards
   */
  let mentalUtilityFn = function(goalCoeffs, mentalRewards, indexOpt) {
    assertEqual(goalCoeffs.length, arrayConcat(mentalRewards).length,
    "mentalUtilityFn(): dimension of goalCoeffs (" +
      toString(goalCoeffs) + ") doesn't match size of mentalRewards ("
    + toString(mentalRewards) + ")")
    let flattenedMentalRewardUtilities =
      arrayConcat(map2(function(rewardUtilFn, rewardsArr) {
        return map(function(reward) {
          return rewardUtilFn(reward)
        }, rewardsArr)
      }, rewardUtilityFunctions.mental, mentalRewards))
    if (isDefined(indexOpt)) {
      assertBetween(indexOpt, 0, goalCoeffs.length,
        "mentalUtilityFn(): indexOpt expected between 0 and " +
      goalCoeffs.length + "; found: " + indexOpt)
      return goalCoeffs[indexOpt] * flattenedMentalRewardUtilities[indexOpt]
    }
    let utility = sum(map2(multiply, goalCoeffs, flattenedMentalRewardUtilities))
    return utility
  }

  // let utilityFnOld = function (goalCoeffs, rewards) {
  //   info("utilityFn(): goalCoeffs: " + arrayToString(goalCoeffs) +
  //     ", rewards: " + arrayToString(rewards))
  //   assertDefined(goalCoeffs, "utilityFn(): goalCoeffs undefined")
  //   assertDefined(rewards, "utilityFn(): rewards undefined")
  //   // TODO: hacky, remove in future
  //   let rewardsNo = sum(map(function(rewardOrArray) {
  //     return _top.Array.isArray(rewardOrArray) ? rewardOrArray.length : 1
  //   }, rewards))
  //   assertEqual(goalCoeffs.length, rewardsNo,
  //     "Dimension of goal coefficients= " + goalCoeffs.length +
  //     " doesn't match rewards= " + rewards.length)
  //   /** TODO: this is hacky and should be fixed */
  //   let physicalNo = params.numberOfRewards.physical
  //   let mentalNo = params.numberOfRewards.mental
  //   let total = physicalNo + mentalNo
  //   assert(total === rewardUtilityFunctions.length,
  //     "utilityFn(): physicalNo + mentalNo = " + total +
  //     ", rewardUtilityfunctions.length = " + rewardUtilityFunctions.length +
  //   "; expected equal")
  //   let physicalRewardUtilities =
  //     map2(apply, rewardUtilityFunctions.slice(0, physicalNo), rewards.slice(0, physicalNo))
  //   let mentalRewardUtilities = (goalCoeffs.length === physicalNo) ? [] :
  //     arrayConcat(map2(function(rewardUtilFn, rewardsArr) {
  //     return map(function(reward) {
  //       return rewardUtilFn(reward)
  //     }, rewardsArr)
  //   }, rewardUtilityFunctions.slice(physicalNo, total), rewards.slice(physicalNo, total)))
  //   let rewardUtilities = physicalRewardUtilities.concat(mentalRewardUtilities)
  //   // let rewardUtilities = map2(apply, rewardUtilityFunctions, rewards)
  //   let result = sum(map2(multiply, goalCoeffs, rewardUtilities))
  //   info("utility computed: " + result)
  //   return result
  // }

  let updatedParams = extend(params, {
    goalCoeffsNumberByAgent,
    beliefRepresentation
  })

  let utilityFns = {
    physical: physicalUtilityFn,
    mental: mentalUtilityFn
    // index: utilityAtIndex
  }

  return {
    actions,
    transitionFn,
    initialState,
    API,
    getPhysicalRewardStructure,
    getMentalRewardStructure,
    utilityFns,
    // utilityAtIndex,
    params: updatedParams
  }
}