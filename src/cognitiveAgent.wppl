/** Cognitive trust aware agent

 * As described in the paper, an agent is characterised by
 * 1. an array of coefficients
 *    *goalCoeffs* = [a1,a2,...,an]
 *    (we require a1 + a2 + ... + an = 1)
 * 2. meta-parameters
 *    *metaParams* = {
 *      alpha: [0,inf),
 *      lookAhead: [0, inf) (integer),
 *      discountFactor: (0,1]
 *    }
 * 2a. whether they use heuristics
 *    *usesHeuristics* : true/false
 * The above dictionaries are stored in a single dict *selfParams*
 * Moreover, each agent has a role (*selfId*), which is 'investor' or 'investee'
 * Next, each agent has an initial state, consisting of
 * 3. initial belief about opponent's mental characteristics.
 *   Currently assumed to be Dirichlet distribution and represented by
 *   its set of parameters. However, implementation should be representation-agnostic:
 *   we provide certain operations on belief:
 *   - updateBelief
 * 4. initial estimations of mental rewards of agent's opponent.
 *    At the minimum, this contains estimation of trust.
 *    It is an array.
 *    Eg [<trustEstimation:distribution>]
 * 5. estimation of opponent's mental parameters
 *    metaParamsEstimations = distribution over values
 {
        alpha: <discrete dist>,
        lookAhead: <discrete dist>,
        discountFactor: <discrete dist>
      }
 Hence, *initialState* takes a form of a following dictionary {
     belief: <3>,
     mentalEstimations: <4>,
     metaParamsEstimations: <5>
   }
 *
 */

/** Creates a cognitive agent with given parameters and initial state
 * participating in a given game as an agent identified by selfId
 *
 * @param selfParams {Object}   Parameters of this agent (see above)
 * @param selfId {Integer}      ID of this agent
 * @param initialState {Object} Initial state of the game
 * @param game {Object}         A big game object which is assumed to come from calling
 *                              makeCSMG
 * @returns {Object}            An object representing this agent, containing its parameters
 *                              (params), functions that describe this agent's decision making
 *                              (act, belief, expectedUtility), functions that compute various
 *                              propoerties of this agent (mentalRewards, mentalState,
 *                              getStateUtility and getActionUtility)
 * */
let makeAgent = function (selfParams, selfId, initialState, game) {
  info("makeAgent() with id " + selfId)
  /** Extract some functions from the game API for easy access */
  let gameAPI = assertDefined(game.API, "makeAgent(): game.API undefined ")
  let isInitial = assertDefined(gameAPI.isInitial,
    "makeAgent(): gameAPI.isInitial undefined")
  let getPreviousState = assertDefined(gameAPI.getPreviousState,
    "makeAgent(): gameAPI.getPreviousState undefined")
  let getLastAction = assertDefined(gameAPI.getLastAction,
    "makeAgent(): gameAPI.getLastAction undefined")
  let endsRound = assertDefined(gameAPI.endsRound,
    "makeAgent(): gameAPI.endsRound undefined")
  let stateToString = assertDefined(gameAPI.stateToString,
    "makeAgent(): gameAPI.stateToString undefined")
  let turn = assertDefined(gameAPI.turn,
    "makeAgent(): gameAPI.turn undefined")
  let actionSimilarity = assertDefined(gameAPI.actionSimilarity,
    "makeAgent(): gameAPI.actionSimilarity undefined")
  info("gameAPI verified")

  /** Extract some fields for easier access */
  let transitionFn = game.transitionFn
  let actions = game.actions
  let utilityFns = assertDefined(game.utilityFns,
    "makeAgent(): game.utilityFn undefined")
  let physicalUtilityFn = assertDefined(utilityFns.physical,
  "makeAgent(): game.utilityFn.physical undefined")
  let mentalUtilityFn = assertDefined(utilityFns.mental,
    "makeAgent(): game.utilityFn.mental undefined")

  info("got transitionFn, actions and utility functions")
  // let utilityFn = game.utilityFn
  // let utilityAtIndex = game.utilityAtIndex

  let gameParams = assertDefined(game.params,
    "makeAgent(): game.params undefined")
  let beliefRepresentation = gameParams.beliefRepresentation
  let numberOfAgents = gameParams.numberOfAgents
  let numberOfRewards = gameParams.numberOfRewards
  let goalCoeffsNumberByAgent = gameParams.goalCoeffsNumberByAgent
  info("game params extracted")

  assertDefined(selfId, "makeAgent(): selfId not given")
  assertBetween(selfId, 0, numberOfAgents - 1,
    "makeAgent(): selfId out of bounds!")

  assertDefined(selfParams, "makeAgent: selfParams not passed")

  /** Goal coefficients of this agent */
  let selfGoalCoeffs = assertDefined(selfParams.goalCoeffs,
    "makeAgent(): selfParams.goalCoeffs undefined")
  assertIsArray(selfGoalCoeffs, NUMBER_TYPE, goalCoeffsNumberByAgent[selfId],
    "makeAgent(): goalCoeffs must be an array of length " +
    goalCoeffsNumberByAgent[selfId] + "; found: " + selfGoalCoeffs.length)
  let selfPhysicalGoalCoeffs = selfGoalCoeffs.slice(0, numberOfRewards.physical)
  let selfMentalGoalCoeffs = selfGoalCoeffs.slice(numberOfRewards.physical)
  info("goal coefficients extracted")

  /** Meta-parameters */
  let selfMetaParams = assertDefined(selfParams.metaParams,
    "makeAgent: metaParams missing")
  assertDefined(selfMetaParams.lookAhead, "makeAgent: lookAhead missing")
  assertDefined(selfMetaParams.alpha, "makeAgent: alpha missing")
  assertDefined(selfMetaParams.discountFactor, "makeAgent: discountFactor missing")
  info("meta parameters extracted")

  /** Initial state */
  assertDefined(initialState, "makeAgent: initialState missing")
  let initialBeliefValue = assertDefined(initialState.belief,
    "makeAgent: initialBelief missing")
  let initialBelief = {
    representation: beliefRepresentation,
    value: initialBeliefValue
  }
  let initialMentalEstimations = assertDefined(initialState.mentalEstimations,
    "makeAgent: initialMentalEstimations missing")
  let metaParamsEstimations = assertDefined(initialState.metaParamsEstimations,
    "makeAgent: metaParamsEstimations missing")
  assertDefined(metaParamsEstimations.alpha,
    "makeAgent: metaParamsEstimations.alpha missing")
  assertIsArray(metaParamsEstimations.alpha, OBJECT_TYPE, numberOfAgents,
    "makeAgent: metaParamsEstimations.alpha is not as expected: "
    + toString(metaParamsEstimations.alpha))
  assertDefined(metaParamsEstimations.lookAhead,
    "makeAgent: metaParamsEstimations.lookAhead missing")
  assertIsArray(metaParamsEstimations.lookAhead, OBJECT_TYPE, numberOfAgents,
    "makeAgent: metaParamsEstimations.lookAhead is not as expected: "
    + toString(metaParamsEstimations.lookAhead))
  assertDefined(metaParamsEstimations.discountFactor,
    "makeAgent: metaParamsEstimations.discountFactor missing")
  assertIsArray(metaParamsEstimations.discountFactor, OBJECT_TYPE, numberOfAgents,
    "makeAgent: metaParamsEstimations.discountFactor is not as expected: "
    + toString(metaParamsEstimations.discountFactor))
  info("initial state extracted")

  /** Reward structures */
  let getPhysicalRewardStructure = game.getPhysicalRewardStructure
  let physicalRewardStructure = assertDefined(getPhysicalRewardStructure(),
    'physicalRewardStructure undefined')
  let stateRewards = physicalRewardStructure.stateRewards
  let actionRewards = physicalRewardStructure.actionRewards

  let getMentalRewardStructure = game.getMentalRewardStructure
  let mentalRewardStructure = assertDefined(getMentalRewardStructure(initialMentalEstimations, selfId),
    'mentalRewardStructure undefined')
  let computeMentalRewards = mentalRewardStructure.computeMentalRewards
  let computeMentalState = mentalRewardStructure.computeMentalState
  let estimateMentalState = mentalRewardStructure.estimateMentalState
  info("reward structures extracted")

  /** Action Heuristic (may be undefined) */
  let heuristics = assertDefined(game.heuristics,
    "makeAgent: game.heuristics must be defined; make sure to include " +
    "it in the specification of your model and call makeCSMG")
  let actionHeuristic = assertDefined(heuristics.action,
    "makeAgent: game.heuristics.action must be defined; found undefined")
  let computeOpponentAction = actionHeuristic.computeOpponentAction
  let beliefHeuristic = heuristics.belief
  let usesHeuristic = assertDefined(selfParams.usesHeuristics,
    "makeAgent: selfParams.usesHeuristics must be passed but found undefined")

  /** Optional: agent game specific parameters */
  // let numberOfAgentParams = gameParams.numberOfAgentParams
  // if (numberOfAgentParams !== undefined) {
  //   let agentParams = selfParams.agentParams
  // }

  /*********************
   * Belief operations *
   *********************
   Abstractly, belief is a continuous probability distribution.
   Concretely, two finite representations are proposed:
   - a discrete distribution that approximates a continuous distribution
   - a set of parameters of dirichlet distribution; here we make an
   assumption that belief follows dirichlet dist
   We would like to be able to change between those, and possibly more,
   representations easily, while exposing a uniform interface for the
   rest of the code that uses belief.
   We therefore include the following methods in the 'interface' of belief
   - belief(state)
   - updateBelief(belief, observation)
   */

  /** Belief
   *  Computes selfID's belief of ofAgentID in *state*. Proceeds recursively.
   *  To avoid recomputation, this function is cached.
   */
  let belief = dp.cache(function (state) {
    info("belief(): state: " + stateToString(state) + ", of agent " + selfId)
    let opponentsHaveNoMoreThanOneReward = all(function(elem) { return elem <= 1 },
      arrayReplace(goalCoeffsNumberByAgent, selfId, 0))
    if (isInitial(state) || opponentsHaveNoMoreThanOneReward) {
      info("belief(): returning initial")
      return initialBelief
      // return retrieveBeliefOf(ofAgentId, initialBelief)
    }
    let prevState = getPreviousState(state)
    let lastAction = getLastAction(state)
    let prevBelief = belief(prevState)
    globalStore.indent += 2
    let result = updateBelief(prevBelief, prevState, lastAction)
    globalStore.indent -= 2
    return result
  })

  /** belief here is over individual agent already */
  let updateBelief = function (belief, state, action) {
    let updateBeliefDiscrete = function (beliefVal, actingAgentID) {
      info(stringInABox("updateBeliefDiscrete(): action=" + action))
      let updatedIndividualBelief = Infer({method: 'enumerate'}, function () {
        let beliefOverActingAgent = retrieveBeliefOver(actingAgentID, belief)
        let goalCoeffs = sampleBelief(beliefOverActingAgent)
        let cond = {
          representation: belief.representation,
          value: goalCoeffs
        }
        // let act = agents[agentID].act
        let predictedAction = sample(act(state, cond))
        factor(actionSimilarity(state, predictedAction, action))
        return goalCoeffs
      })
      /** update belief over that agent */
      let updatedBeliefValue = arrayReplace(belief.value, actingAgentID, updatedIndividualBelief)
      return {
        representation: 'discrete',
        value: updatedBeliefValue
      }
      /** for now lets assume we dont update belief */
      // return belief
    }

    /**
     * *belief* here is an array of parameters (of a dirichlet distribution)
     * The idea of an update is as follows: for each reward (physical or mental)
     * we compute the probability of agent taking *action* assuming they're solely
     * motivated by that reward. We then increment the distribution parameter corresponding
     * to that reward by the value of probability computed.
     */
    let updateBeliefDirichlet = function (beliefVal, actingAgentID) {
      info("updateBeliefDirichlet(): actingAgentID=" + actingAgentID + ", belief:")
      info(arrayToString(beliefVal))
      /** compute the action under each possible reward, i.e. conditional action */
      let computeActionDist = function (index) {
        /** A 'conditional' call to act() in a sense that certain conditions are
         * placed on the execution of that call. In particular:
         * - value of trust computed based on *belief* is to be used for the purposes
         *    of computing utility of this agent in recursive act calls
         * - opponent is assumed to only care about reward number *index*
         */
        let cond = {
          representation: 'dirichlet',
          value: index
        }
        // let act = agents[agentID].act
        return act(state, cond) /** index is the condition */
      }
      let actingAgentsGoalCoeffIndexes =
        rangeArray(0, goalCoeffsNumberByAgent[actingAgentID]-1)
      let actionDists = map(computeActionDist, actingAgentsGoalCoeffIndexes)
      let actionProbs = map(function (actionDist) {
        return Math.exp(actionDist.score(action))
      }, actionDists)
      // action probs might be very small; instead of adding them directly, we use them
      // as proportions and add a total of one
      let actionProbsSum = sum(actionProbs)
      let actionProbsNormalised = map(function (prob) {
        return (actionProbsSum === 0) ? 0 : 2 * prob / actionProbsSum
      }, actionProbs)
      assert(actionProbsSum === 0 || approxEqual(sum(actionProbsNormalised), 2),
        "Normalised action probabilities in updateBeliefDirichlet " +
        "don't sum to 1; found: " + sum(actionProbsNormalised))
      let updatedBeliefOverActingAgent =
        map2(add, beliefVal[actingAgentID], actionProbsNormalised)
      let updatedBelief = arrayReplace(beliefVal, actingAgentID, updatedBeliefOverActingAgent)
      info("updateBeliefDirichlet(): returning updatedBelief (value): ")
      info(arrayToString(updatedBelief))
      return {
        representation: 'dirichlet',
        value: updatedBelief
      }
    }

    info("updateBelief(): belief=" + toString(belief) + ", state=" +
      stateToString(state) + ", action=" + toString(action))
    let appropriateFn = {
      'discrete': updateBeliefDiscrete,
      'dirichlet': updateBeliefDirichlet
    }[belief.representation]

    let actingAgentID = turn(state)
    if (selfId == actingAgentID || actions(state).length == 1) {
      return belief
    }
    if (apply3(beliefHeuristic.applies, state, action, selfId)) {
      return apply3(beliefHeuristic.updateBelief, belief, state, action)
    }
    return appropriateFn(belief.value, actingAgentID)
    /** again, for now assume no update for simplicity */
    // return belief
  }

  /** Utility function
   * Computes utility for agent *role* when *action* is performed (by whoever) in *state*.
   * Physical rewards are computed using physical reward structure
   * 1. If utility of this agent is being computed, mental rewards are computed
   * using mental rewards structure (dynamics functions) and goal coefficients are known
   * 2. Otherwise, there are two cases:
   * 2a. When no condition *cond* is passed then opponent's utility is computed with respect
   *     to this agent's belief and mental rewards are taken as real values this agent knows
   * 2b. When condition *cond* is passed, utility is computed wrt
   */
  let actionUtility = (function (state, action, ofAgentID, cond, mentalSnapshot) {
    info("actionUtility(state: " + stateToString(state) + ", action: " + action +
      ", role: " + ofAgentID + ", cond: " + condToString(cond) + ")")
    assertDefined(mentalSnapshot, "actionUtility: mentalSnapshot undefined")

    let physicalRewards = actionRewards(state, action)[ofAgentID]
    // let mentalRewardsDummy = mentalUtilities[ofAgentID]
    // let mentalRewardsDummy = repeat(numberOfRewards.mental, function () {
    //   return 0
    // })
    if (ofAgentID == selfId) {
      // case 1
      info("actionUtility(): case 1: agent computes their own utility")
      return physicalUtilityFn(selfPhysicalGoalCoeffs, physicalRewards)
    }
    if (cond !== undefined) {
      // case 2b
      info("actionUtility(): case 2b: agent computes other's utility on cond")
      return physicalCondUtilityFn(physicalRewards, cond, utilityFns)
    }
    info("actionUtility(): case 2a: agent computes other's utility")
    // TODO: which one should be used?
    // var belief = belief(state)
    let fullBelief = belief(mentalSnapshot.state)
    let individualBelief = retrieveBeliefOver(ofAgentID, fullBelief)

    // case 2a
    let expectations =
      goalCoeffsExpectation(individualBelief, goalCoeffsNumberByAgent[ofAgentID]).slice(0, numberOfRewards.physical)
    return physicalUtilityFn(expectations, physicalRewards)
  })

  /** This agent (selfID) computes state utility of agent ofAgentID,
   * from the perspective of state saved in mentalSnapshot, possibly
   * under condition cond.
   *  @param  {Object}  state     State at which utility is computed
   *  @param  {Integer} ofAgentID ID of an agent whose utility is being computed
   *  @param  {Object}  cond
   *
   *  @return {Number}            Utility
   *
   *  */
  let stateUtility = (function (state, ofAgentID, cond, mentalSnapshot) {
    info("stateUtility(): agent " + selfId + " computing state " +
      "utility of " + ofAgentID + " at state " + stateToString(state) +
      " from perspective of state " + stateToString(mentalSnapshot.state) +
      ", under cond: " + condToString(cond) + ")")
    assertDefined(mentalSnapshot,"stateUtility(): mentalSnapshot undefined")
    let stateUtil = function(goalCoeffs, rewards) {
      info("stateUtil(): goalCoeffs=" + toString(goalCoeffs) +
        ", rewards: " + toString(rewards))
      let physicalUtility = physicalUtilityFn(goalCoeffs.physical, rewards.physical)
      let mentalUtility = mentalUtilityFn(goalCoeffs.mental, rewards.mental)
      return physicalUtility + mentalUtility
    }

    /** this is a big decision - whether belief is computed at state or
     * mentalSnapshot.state. The former involves computing future belief
     * while the latter uses current belief (as current state is saved
     * in mentalSnapshot) */
    let belief = belief(mentalSnapshot.state)
    let physicalRewards = stateRewards(state)[ofAgentID]
    let mentalRewards = computeMentalRewards(state, ofAgentID, belief, mentalSnapshot)
    let rewards = {
      physical: physicalRewards,
      mental: mentalRewards
    }
    if (ofAgentID == selfId) {
      info("stateUtility(): " + selfId + " computed rewards of " + ofAgentID +
        ": " + arrayToString(mentalRewards))
      let goalCoeffs = {
        physical: selfPhysicalGoalCoeffs,
        mental: selfMentalGoalCoeffs
      }
      return stateUtil(goalCoeffs, rewards)
    }

    if (cond !== undefined) {
      info("stateUtility(): case 2b")
      let result = conditionalUtility(rewards, cond, utilityFns)
      return result
    }

    // case 2a
    info("stateUtility(): case 2a")
    let ofAgentBelief = retrieveBeliefOver(ofAgentID, belief)
    let expectations = goalCoeffsExpectation(ofAgentBelief, goalCoeffsNumberByAgent[ofAgentID])
    let goalCoeffs = {
      physical: expectations.slice(0, numberOfRewards.physical),
      mental: expectations.slice(numberOfRewards.physical)
    }
    let utility = stateUtil(goalCoeffs, rewards)
    info("stateUtility: returning " + utility)
    return utility
  })

  /** Compute action (distribution) at given state from the
   * perspective of this agent. There are three cases:
   * - agent computes their own action (state belongs to this agent)
   * - agent computes their opponent's action
   * - agent computes their opponent's action under condition
   * It either computes the action (distribution) for this agent (when no @cond
   * passed) or action (distribution) of opponent assuming @cond.
   *
   * @param state
   * @param cond (optional)
   * @return
    a distribution over actions available to the acting agent at @state
   */
  let act = dp.cache(function (state, condOpt) {
    info("act(state: " + stateToString(state) + ", cond: " + condToString(condOpt) + ")")
    let turn = turn(state)
    let thisAgentActs = turn === selfId
    explain("Agent " + selfId + " computes action at state " +
      stateToString(state), thisAgentActs && !INFERENCE_MODE)
    assert(!thisAgentActs || condOpt !== undefined,
      "Input to act() doesn't meet precondition: can't compute " +
      "own action under a condition")
    /** prepare mentalSnapshot for future utility computations) */
    let belief = belief(state)
    let mentalState = mapN(function(i) {
      return computeMentalState(state, belief, i)
    }, numberOfRewards.mental)
    let mentalSnapshot = {
      values: mentalState,
      state
    }
    globalStore.indent = 0
    explain("Agent will explore the game tree up to depth " +
      selfMetaParams.lookAhead, thisAgentActs && !INFERENCE_MODE)
    let actionDist = Infer({method: 'enumerate'}, function () {
      let othersMetaParams = sampleMetaParamsEstimations(metaParamsEstimations)
      let allMetaParams = mergeMetaParams(othersMetaParams, selfMetaParams, selfId)
      let lookAhead = allMetaParams.lookAhead[turn]
      let actionRecDist = actRec(state, lookAhead, allMetaParams, condOpt, mentalSnapshot)
      let action = sample(actionRecDist)
      return action
    })
    let actionDistAsString = discreteDistributionToString(actionDist, actions(state))
    explain("Agent " + selfId + " computed the " +
      "following action distribution:\n" + actionDistAsString, thisAgentActs)
    return actionDist
  })

  /**
   * Params:
   *   @param state a dictionary { turn, returns, investments }
   *   @param timeLeft - an integer specifying time horizon of the decision making
   *   @param allMetaParams - a dict consisting of arrays of opponents'
   *   @param cond - is an optional condition, used for belief update
   *   @param mentalSnapshot - records mental state of this agent at the point
   *      when action is being computed
   */
  let actRec = (function (state, timeLeft, allMetaParams, cond, mentalSnapshot) {
    info(spaces(globalStore.indent) + "actRec(): state=" + stateToString(state) +
      ", horizon: " + timeLeft + ")", !INFERENCE_MODE)
    let turn = turn(state)
    let alpha = assertDefined(allMetaParams.alpha[turn],
      "bad game specification: agent's " + selfId + " estimation " +
      "of agent's " + turn + " alpha undefined")
    let thisAgentActs = turn === selfId
    let availableActions = assertDefined(actions(state),
      "bad game specification: actions at state " +
      stateToString(state) + " undefined")
    // if (state.investments.length === 0 && state.returns.length === 0) {
    //   display("av actions")
    //   display(availableActions)
    // }
    assertIsArray(availableActions, ANY_TYPE, -1,
      "bad game specification: actions at state " +
      stateToString(state) + " are not an array, found: " +
      toString(availableActions))
    let actionDist = Infer({method: 'enumerate'}, function () {
      let action = uniformDraw(availableActions)
      if (availableActions.length === 1) return action
      // use heuristic if (i) one is defined, (ii) it's opponent's turn and
      // (iii) we're not updating belief
      let useHeuristic = !thisAgentActs && usesHeuristic && apply3(actionHeuristic.applies, state, selfId, turn) && !cond
      let actionLikelihood = useHeuristic ?
        computeOpponentAction(selfId, turn, action, state, mentalState, mentalEstimation) :
        expectedUtility(state, action, turn, timeLeft, allMetaParams, cond, mentalSnapshot)
      factor(alpha * actionLikelihood)
      return action
    })
    debug("Computed following dist at state " + stateToString(state) +
      " with " + timeLeft + " steps left:\n" +
      discreteDistributionToString(actionDist, availableActions) + "\n" + globalStore.indent,
      !INFERENCE_MODE && globalStore.indent == 2 && turn !== selfId)
    return actionDist
  })

  /** Computes expected utility of agent with *role* upon *action* taken in *state*
   * with *timeLeft* left. *timeLeft* must be > 0!
   * Params:
   *   - *state* - a dictionary describing state, i.e. turn, returns and investments
   *   - *action* - a string identifying action taken in *state*
   *   - *role* - a string identifying the agent whose expected utility is to
   be computed
   *   - *timeLeft* - integer
   *   - *otherParams* -
   */
  let expectedUtility = (
    function (state, action, ofAgentID, timeLeft, allMetaParams, cond, mentalSnapshot) {
      info(spaces(globalStore.indent) + "expectedUtility() at state " +
        stateToString(state) + ", for action: " + action + ", computed by " +
        selfId + " for " + ofAgentID + ", with time horizon " + timeLeft +
        ", condition " + (cond === undefined ? "undefined" : cond.value), !INFERENCE_MODE)
      if (timeLeft === 0) return 0
      let u = actionUtility(state, action, ofAgentID, cond, mentalSnapshot)
      let nextTimeLeft = timeLeft - 1
      let discountFactor = assertDefined(allMetaParams.discountFactor[ofAgentID],
        "bad game specification: agent's " + selfId + " estimation " +
        "of agent's " + turn + " discountFactor undefined")
      globalStore.indent += 2
      let futureUtilityDist = Infer({method: 'enumerate'}, function () {
        let nextStateDist = transitionFn(state, action)
        assertHasType(nextStateDist, DIST_TYPE,
          "bad game specification: transitionFn must return a distribution, but " +
          "returned " + toString(nextStateDist) + " when called at state " +
          stateToString(state) + " with action " + action)
        let nextState = sample(nextStateDist)
        let nextStateUtility = stateUtility(nextState, ofAgentID, cond, mentalSnapshot)
        if (nextTimeLeft == 0) {
          return nextStateUtility
        }
        let nextTurn = turn(nextState)
        let nextActionTakerHorizon = min(allMetaParams.lookAhead[nextTurn], nextTimeLeft)
        let nextActionDist = actRec(nextState, nextActionTakerHorizon, allMetaParams, cond, mentalSnapshot)
        let nextAction = sample(nextActionDist)
        return nextStateUtility + expectedUtility(nextState, nextAction,
          ofAgentID, nextTimeLeft, allMetaParams, cond, mentalSnapshot)
      })
      globalStore.indent -= 2
      let futureUtilExp = expectation(futureUtilityDist)
      let discount = endsRound(state, action) ? discountFactor : 1
      let eu = u + discount * futureUtilExp
      debug("EU of " + ofAgentID + " at state " + stateToString(state) +
        " for action " + action + ", with " + timeLeft + " steps left: " + eu,
        (!INFERENCE_MODE && (ofAgentID === selfId && cond === undefined && globalStore.indent === 0)))
      return eu
    })

  let mentalRewards = function(state) {
    let belief = belief(state)
    // TODO: maybe change that later
    return arrayConcat(computeMentalRewards(state, selfId, belief))
  }

  let mentalState = function(state, mentalAttitudeIndex) {
    assertDefined(state, "mentalState(): state undefined")
    assertDefined(mentalAttitudeIndex, "mentalState(): mentalAttitudeIndex undefined")
    let belief = belief(state)
    return computeMentalState(state, belief, mentalAttitudeIndex)
  }

  let mentalEstimation = function(state, ofAgentID, mentalAttitudeIndex) {
    assertDefined(state, "mentalEstimation(): state undefined")
    assertDefined(ofAgentID, "mentalEstimation(): ofAgentID undefined")
    assertDefined(mentalAttitudeIndex, "mentalEstimation(): mentalAttitudeIndex undefined")
    return estimateMentalState(state, ofAgentID, mentalAttitudeIndex)
  }

  let getStateUtility = function(state) {
    return stateUtility(state, selfId, undefined, { state })
  }

  let getActionUtility = function(state, action) {
    return actionUtility(state, action, selfId, undefined, { state })
  }

  return {
    params: selfParams,
    act,
    expectedUtility,
    belief,
    mentalRewards,
    mentalState,
    mentalEstimation,
    getStateUtility,
    getActionUtility
  }
}