let getGameAPI = function (gameSpecificAPI) {

  let stateToString = gameSpecificAPI.stateToString

  let mentalSnapshotToString = function (mentalSnapshot) {
    let state = stateToString(mentalSnapshot.state)
    let values = mentalSnapshot.values
    return "{state: " + state + ", values: " + values + "}"
  }

  /** This implements default action similarity measure (which we could
   * call 'discrete measure') in case a measure is not provided in
   * user game specification */
  let actionSimilarity = function(state, a1, a2) {
    assertDefined(state, "actionSimilarity(): state undefined")
    assertDefined(a1, "actionSimilarity(): a1 undefined")
    assertDefined(a2, "actionSimilarity(): a2 undefined")
    let actionSimilarityOpt = gameSpecificAPI.actionSimilarity
    if (isDefined(actionSimilarityOpt)) return actionSimilarityOpt(state, a1,a2)
    if (_.isEqual(a1,a2)) return 0
    return -100
  }

  let APIExtension = {
    mentalSnapshotToString,
    actionSimilarity
  }

  let gameAPI = extend(gameSpecificAPI, APIExtension)
  return gameAPI
}


/** makeCSMG (cognitive stochastic multiplayer game)
 * @param gameSetup
 *  an object of the form
 *  {
    actions,
    transition,
    initialState,
    API,
    getPhysicalRewards,
    getMentalStateDynamics,
    utilityFn,
    params
    }
 * This object contains all the game specific information needed
 * to create a CSMG and it is defined by the user,
 * @param externalParams
 * of the form
 * {
 *   beliefRepresentation
 * }
 *
 * Given a game configuration, this generic function creates a
 * cognitive model. This mostly involves creating an appropriate
 * mental reward structure.
 */
let makeCSMG = function (gameSetup, externalParams) {
  /** Extract basic components and validate user-defined *gameSetup* */
  assertDefined(gameSetup,
    "makeCSMG(): gameSetup undefined")
  assertDefined(externalParams,
    "makeCSMG(): externalParams undefined")
  let params = assertDefined(gameSetup.params,
    "makeCSMG(): gameSetup.params undefined")
  let beliefRepresentation = assertDefined(externalParams.beliefRepresentation,
    "makeCSMG(): params.beliefRepresentation undefined")
  let numberOfAgents = assertDefined(params.numberOfAgents,
    "makeCSMG(): params.numberOfAgents undefined")
  let numberOfRewards = assertDefined(params.numberOfRewards,
    "makeCSMG(): params.numberOfRewards undefined")
  assertDefined(numberOfRewards.physical,
    "makeCSMG(): numberOfRewards.physical undefined")
  assertDefined(numberOfRewards.mental,
    "makeCSMG(): numberOfRewards.mental undefined")

  let actions = assertDefined(gameSetup.actions,
    "makeCSMG(): gameSetup.actions undefined")
  let transitionFn = assertDefined(gameSetup.transitionFn,
    "makeCSMG(): gameSetup.transitionFn undefined")
  let initialState = assertDefined(gameSetup.initialState,
    "makeCSMG(): gameSetup.params undefined")

  /** Extract game-specific API calls for easy access */
  let API = getGameAPI(gameSetup.API)

  let getPreviousState = assertDefined(API.getPreviousState,
    "makeCSMG(): API.getPreviousState undefined")
  let getLastAction = assertDefined(API.getLastAction,
    "makeCSMG(): API.getLastAction undefined")
  let isInitial = assertDefined(API.isInitial,
    "makeCSMG(): API.isInitial undefined")
  let stateToString = assertDefined(API.stateToString,
    "makeCSMG(): API.stateToString undefined")

  /** Reward structures */
  let physicalRewardStructure = assertDefined(gameSetup.physicalRewardStructure,
    "makeCSMG(): gameSetup.physicalRewardStructure undefined")
  let mentalStateDynamics = assertDefined(gameSetup.mentalStateDynamics,
    "makeCSMG(): gameSetup.mentalStateDynamics undefined")
  let estimationHeuristicsArr = assertDefined(mentalStateDynamics.estimationHeuristicArr,
    "makeCSMG(): estimationHeuristicsArr undefined")
  let mentalStateArr = assertDefined(mentalStateDynamics.mentalStateArr,
    "makeCSMG(): mentalStateArr undefined")
  let mentalUtilities = assertDefined(mentalStateDynamics.mentalUtilities,
    "makeCSMG(): mentalUtilities undefined")
  assertIsArray(estimationHeuristicsArr, FUNCTION_TYPE, numberOfRewards.mental,
    "makeCSMG(): estimationHeuristicArr: " + arrayToString(estimationHeuristicsArr))
  assertIsArray(mentalStateArr, FUNCTION_TYPE, numberOfRewards.mental,
    "makeCSMG(): mentalStateArr: " + arrayToString(mentalStateArr))
  assertIsArray(mentalUtilities, ARRAY_TYPE, numberOfAgents,
    "makeCSMG(): mentalUtilities: " + arrayToString(mentalUtilities))

  /** different agents are motivated by different things so number of
   * goal coefficients generally differs between agents */
  let goalCoeffsNumberByAgent =
    computeGoalCoeffsNumber(numberOfRewards.physical, mentalUtilities)


  /** Reward Utilities */
  let rewardUtilityFunctions = assertDefined(gameSetup.rewardUtilityFunctions,
    "makeCSMG(): gameSetup.rewardUtilityFunctions undefined")
  assertDefined(rewardUtilityFunctions.physical, "makeCSMG():" +
    "gameSetup.rewardsUtilityFunctions.physical undefined")
  assertDefined(rewardUtilityFunctions.mental, "makeCSMG():" +
    "gameSetup.rewardsUtilityFunctions.mental undefined")
  assertEqual(rewardUtilityFunctions.physical.length, numberOfRewards.physical,
    "makeCSMG(): number of physical reward utility functions " +
    "expected to match number of physical rewards; found: " +
    rewardUtilityFunctions.physical.length + ", " + numberOfRewards.physical)
  assertEqual(rewardUtilityFunctions.mental.length, numberOfRewards.mental,
    "makeCSMG(): number of mental reward utility functions " +
    "expected to match number of mental rewards; found: " +
    rewardUtilityFunctions.mental.length + ", " + numberOfRewards.mental)

  /** Heuristics */
  let heuristics = assertDefined(gameSetup.heuristics,
    "makeCSMG(): gameSetup.heuristic undefined")
  assertDefined(heuristics.action, "makeCSMG(): gameSetup.heuristics.action undefined")
  assertDefined(heuristics.belief, "makeCSMG(): gameSetup.heuristics.belief undefined")
  assertDefined(heuristics.action.applies,
    "makeCSMG(): gameSetup.heuristics.action.applies undefined")
  assertDefined(heuristics.belief.applies,
    "makeCSMG(): gameSetup.heuristics.belief.applies undefined")

  /** Physical rewards structure
   *  This simply delegates to the physicalRewards object as given
   *  in gameSetup
   */
  let getPhysicalRewardStructure = function () {
    return physicalRewardStructure
  }

  /** Mental reward structure uses mental state dynamics provided in
   * @gameSetup to enable computation of mental rewards in any state.
   * Primarily, this is used by an agent to estimate mental state of
   * their opponent.
   * However, it is also used by an agent to estimate their own mental
   * state at some future point, thereby not requiring to compute belief
   * of that agent.
   *
   * @param initialEstimations (@type array of array of distributions,
   * with holes, indexed by agent)
   *  initial mental state estimations of this agent
   * @param selfAgentID
   *  identifier (nonnegative integer, usually 0 or 1) of an agent
   *  identifies the agent for which this reward structure is provided
   */
  let getMentalRewardStructure = function (initialEstimations, selfAgentID) {
    /**
     * @type array of update functions (each of @type (value, agent, state, action) -> newValue)
     */

    /** Represents @selfAgentID computing mental rewards of @ofAgentID.
     *
     * There are two cases:
     * - if @mentalSnapshot is missing, @agentID is computing its own mental rewards
     *   as part of selecting its best action (expected utility)
     * - if @mentalSnapshot is passed, @agentID is computing its opponent's mental
     *   rewards, either as part of selecting an action or updating belief
     * @param state
     * @param ofAgentID
     * @param belief
     * @param mentalSnapshot (optional)
     *  a reference point from which to use mental state dynamics
     *  it's an object { state, values }
     *  (it snapshots mental state values @estimations at @state)
     *  if omitted, initial state with initial estimations serves as snapshot
     *  snapshot is not used when this agent's mental rewards are being computed
     *
     *  @returns [<rewards for mental state 0>, <rewards for mental state 1>, ...]
     */
    let computeMentalRewards = function (state, ofAgentID, belief, mentalSnapshot) {
      info("computeMentalRewards(): " + selfAgentID + " computing mental " +
        "rewards of " + ofAgentID + " at state " + stateToString(state))

      // precompute mental state of selfAgentID; not strictly necessary,
      // as not all the values will be used
      // let mentalState = computeMentalState(state, belief)
      // info("computeMentalRewards(): mental state computed: " + arrayToString(mentalState))
      /** ofAgentMentalUtility is an array with one elem per mental attitude
       * and that elem is an array which identifies agents whose attitude
       * (this particular one) ofAgentID cares about */
      let ofAgentMentalUtility = mentalUtilities[ofAgentID]
      let computeRewardsFromAttitude = function(mentalAttitudeIndex, agentArr) {
        /** selfAgentID estimates mental reward gained by ofAgentID from
         * overAgentID's mental state  */
        let estimateReward = function(overAgentID) {
          if (selfAgentID === overAgentID) {
            if (selfAgentID === ofAgentID)
              return computeMentalState(state, belief, mentalAttitudeIndex)
              // return mentalState[mentalAttitudeIndex]
            else
              return nestedEstimation(state, mentalAttitudeIndex, mentalSnapshot)
          }
          return expectation(estimateMentalState(state, overAgentID, mentalAttitudeIndex))
        }
        return map(estimateReward, agentArr)
      }
      let rewardsByAttitudeArr = mapIndexed(computeRewardsFromAttitude, ofAgentMentalUtility)
      return rewardsByAttitudeArr
    }

    let computeMentalState = function(state, belief, mentalAttitudeIndex) {
      info("mentalState() of agent " + selfAgentID + " at state " +
        stateToString(state) + " for mental attitude " + mentalAttitudeIndex)
      let mentalStateComputeFn = mentalStateArr[mentalAttitudeIndex]
      let result = mentalStateComputeFn(selfAgentID, belief, state)
      info("mentalState(): computed " + result)
      return result
    }

    /** returns the estimated value of this agent's mental state (identified
     * by *mentalAttitudeIndex*), computed using mental state dynamics
     * relative to a true value at some past state saved in *mentalSnapshot*
     * */
    let nestedEstimation = function(state, mentalAttitudeIndex, mentalSnapshot) {
      info("nestedEstimation(state=" + stateToString(state) + ", index=" +
        mentalAttitudeIndex + ")")
      assertDefined(state,
        "nestedEstimation(): state undefined!")
      assertDefined(mentalAttitudeIndex,
        "nestedEstimation(): mentalAttitudeIndex undefined!")
      assertDefined(mentalSnapshot,
        "nestedEstimation(): mentalSnapshot undefined!")
      assertDefined(mentalSnapshot.state,
        "nestedEstimation(): mentalSnapshot.state undefined!")
      assertDefined(mentalSnapshot.values,
        "nestedEstimation(): mentalSnapshot.values undefined!")
      let snapState = mentalSnapshot.state
      let snapValues = mentalSnapshot.values
      let updateFn = estimationHeuristicsArr[mentalAttitudeIndex]
      /** Local recursive function to save stack space by passing less parameters */
      let nestedEstimationRec = function(state) {
        if (_.isEqual(state, snapState)) {
          return snapValues[mentalAttitudeIndex]
        }
        let prevState = getPreviousState(state)
        // let lastAction = getLastAction(state)
        let prevValue = nestedEstimationRec(prevState)
        let curValue = updateFn(prevValue, selfAgentID, selfAgentID, state)
        return curValue
      }
      let result = nestedEstimationRec(state)
      info("nestedEstimation(): returning")
      return result
    }


    /** computes this agent's (selfAgentID) estimation of *ofAgentID*
     * mental state (attitude *rewardIndex*)
     * @param state {State}
     * @param ofAgentID {Int}
     * @param rewardIndex {Int}
     * @returns mentalStateEstimation {Distribution} */
    let estimateMentalState =
        dp.cache(function(state, ofAgentID, rewardIndex) {
        info("estimation(): state=" + stateToString(state) +
          ", ofAgentID=" + ofAgentID + ", rewardIndex=" + rewardIndex)
        if (isInitial(state)) {
          assert(initialEstimations[ofAgentID] !== undefined &&
            initialEstimations[ofAgentID][rewardIndex] !== undefined,
            "initial estimations of agent " + selfAgentID +
            " about " + ofAgentID + " on mental reward " + rewardIndex +
            " requested, but undefined")
          return initialEstimations[ofAgentID][rewardIndex]
        }
        let prevState = getPreviousState(state)
        let prevEstimation = estimateMentalState(prevState, ofAgentID, rewardIndex)
        let updateEstimation = function (updateFn, prevEstimation) {
          return Infer({method: 'enumerate'}, function () {
            assertHasType(prevEstimation, DIST_TYPE,
              "estimation(): " + selfAgentID + "'s initial " +
              "estimation of " + ofAgentID + "'s mental state " +
              rewardIndex + " is not a distribution: it's " +
              toString(prevEstimation))
            let prevValue = sample(prevEstimation)
            return updateFn(prevValue, selfAgentID, ofAgentID, state)
          })
        }
        // let curEstimations = map2(updateEstimation, estimationHeuristicsArr, prevEstimations)
        let result = updateEstimation(estimationHeuristicsArr[rewardIndex], prevEstimation)
        info("estimation(): state=" + stateToString(state) +
          "; returning " + result)
        return result
      })

    return {
      computeMentalRewards,
      computeMentalState,
      estimateMentalState
    }
  }

  /** Computes mental utility given a nested array of rewards
   * (mentalRewards) which is indexed by mentalAttitude
   *
   * @param goalCoeffs      {Array<Double>}
   * @param physicalRewards {Array<Double>}
   * @param indexOpt        {Int}            (optional)
   */
  let physicalUtilityFn = function(goalCoeffs, physicalRewards, indexOpt) {
    assertEqual(goalCoeffs.length, physicalRewards.length,
      "physicalUtilityFn(): goalCoeffs (" + toString(goalCoeffs)
      + ") dimensions don't match physicalRewards (" + toString(physicalRewards)
      + ") dimensions")
    assertEqual(goalCoeffs.length, rewardUtilityFunctions.physical.length,
      "physicalUtilityFn(): goalCoeffs (" + toString(goalCoeffs)
      + ") dimensions don't match rewardUtilityFunctions.physical " +
      "dimensions: " + rewardUtilityFunctions.physical.length)
    info("physicalUtilityFn(): goalCoeffs=" + toString(goalCoeffs) +
    ", physicalRewards=" + toString(physicalRewards) + ", indexOpt=" + indexOpt)
    if (isDefined(indexOpt)) {
      assertBetween(indexOpt, 0, goalCoeffs.length-1,
        "physicalUtilityFn(): index out of bounds: " + indexOpt +
      "; expected between 0 and " + (goalCoeffs.length - 1))
      return goalCoeffs[indexOpt] * apply1(rewardUtilityFunctions.physical[indexOpt], physicalRewards[indexOpt])
    }
    let physicalRewardUtilities =
      map2(apply1, rewardUtilityFunctions.physical, physicalRewards)
    let utility = sum(map2(multiply, goalCoeffs, physicalRewardUtilities))
    return utility
  }

  /** Computes mental utility given a nested array of rewards
   * (mentalRewards) which is indexed by mentalAttitude
   *
   * @param goalCoeffs {Array<Double>}
   * @param mentalRewards {Array<Double>}
   * @param indexOpt {Int} (optional) must be passed relative to mental rewards
   */
  let mentalUtilityFn = function(goalCoeffs, mentalRewards, indexOpt) {
    assertEqual(goalCoeffs.length, arrayConcat(mentalRewards).length,
    "mentalUtilityFn(): dimension of goalCoeffs (" +
      toString(goalCoeffs) + ") doesn't match size of mentalRewards ("
    + toString(mentalRewards) + ")")
    info("mentalUtilityFn(): goalCoeffs=" + toString(goalCoeffs) +
      ", mentalRewards=" + toString(mentalRewards) + ", indexOpt=" + indexOpt)
    let flattenedMentalRewardUtilities =
      arrayConcat(map2(function(rewardUtilFn, rewardsArr) {
        return map(function(reward) {
          return rewardUtilFn(reward)
        }, rewardsArr)
      }, rewardUtilityFunctions.mental, mentalRewards))
    if (isDefined(indexOpt)) {
      assertBetween(indexOpt, 0, goalCoeffs.length,
        "mentalUtilityFn(): indexOpt expected between 0 and " +
      goalCoeffs.length + "; found: " + indexOpt)
      return goalCoeffs[indexOpt] * flattenedMentalRewardUtilities[indexOpt]
    }
    let utility = sum(map2(multiply, goalCoeffs, flattenedMentalRewardUtilities))
    return utility
  }

  let updatedParams = extend(params, {
    goalCoeffsNumberByAgent,
    beliefRepresentation
  })

  let utilityFns = {
    physical: physicalUtilityFn,
    mental: mentalUtilityFn
    // index: utilityAtIndex
  }

  return {
    actions,
    transitionFn,
    initialState,
    API,
    getPhysicalRewardStructure,
    getMentalRewardStructure,
    utilityFns,
    heuristics,
    // utilityAtIndex,
    params: updatedParams
  }
}